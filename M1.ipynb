{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "assert sys.version_info >= (3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "#check os.environ ld_library_path is the same here as when I do it in python via terminal, if I get issues\n",
    "\n",
    "#sometimes I can't select the GPU. In this case, try: https://forums.fast.ai/t/tip-limiting-tensorflow-to-one-gpu/1995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = keras.backend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = keras.datasets.mnist\n",
    "(X_train_full, y_train_full), (X_test,y_test) = mnist.load_data()\n",
    "\n",
    "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
    "\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "#1000 labelled points in train set for this example\n",
    "\n",
    "X_train_la = X_train[:1000]\n",
    "X_train_un = X_train[1000:]\n",
    "\n",
    "y_train_la = y_train[:1000]\n",
    "y_train_un = np.empty((X_train_la.shape[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "class Sampling(keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        mean, log_var = inputs\n",
    "        return K.random_normal(tf.shape(log_var)) * K.exp(log_var/2) + mean\n",
    "\n",
    "codings_size = 50\n",
    "\n",
    "inputs = keras.layers.Input(shape=[28, 28])\n",
    "z = keras.layers.Flatten()(inputs)\n",
    "z = keras.layers.Dense(600, activation=\"softplus\")(z)\n",
    "z = keras.layers.Dense(300, activation=\"softplus\")(z)\n",
    "codings_mean = keras.layers.Dense(codings_size)(z)\n",
    "codings_log_var = keras.layers.Dense(codings_size)(z)\n",
    "codings = Sampling()([codings_mean, codings_log_var])\n",
    "variational_encoder = keras.models.Model(\n",
    "    inputs=[inputs], outputs=[codings_mean, codings_log_var, codings,z])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = keras.layers.Input(shape=[codings_size])\n",
    "x = keras.layers.Dense(300, activation=\"softplus\")(decoder_inputs)\n",
    "x = keras.layers.Dense(600, activation=\"softplus\")(x)\n",
    "x = keras.layers.Dense(28 * 28, activation=\"sigmoid\")(x)\n",
    "outputs = keras.layers.Reshape([28, 28])(x)\n",
    "variational_decoder = keras.models.Model(inputs=[decoder_inputs], outputs=[outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, codings,last_hidden_layer = variational_encoder(inputs)\n",
    "reconstructions = variational_decoder(codings)\n",
    "variational_ae = keras.models.Model(inputs=[inputs], outputs=[reconstructions])\n",
    "\n",
    "latent_loss = -0.5 * K.sum(\n",
    "    1 + codings_log_var - K.exp(codings_log_var) - K.square(codings_mean),\n",
    "    axis=-1)\n",
    "variational_ae.add_loss(K.mean(latent_loss) / 784)\n",
    "#question on how loss is computed too..\n",
    "variational_ae.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/200\n",
      "55000/55000 [==============================] - 8s 145us/sample - loss: 0.2110 - val_loss: 0.1755\n",
      "Epoch 2/200\n",
      "55000/55000 [==============================] - 6s 117us/sample - loss: 0.1671 - val_loss: 0.1573\n",
      "Epoch 3/200\n",
      "55000/55000 [==============================] - 6s 115us/sample - loss: 0.1546 - val_loss: 0.1497\n",
      "Epoch 4/200\n",
      "55000/55000 [==============================] - 6s 112us/sample - loss: 0.1493 - val_loss: 0.1467\n",
      "Epoch 5/200\n",
      "55000/55000 [==============================] - 6s 114us/sample - loss: 0.1468 - val_loss: 0.1446\n",
      "Epoch 6/200\n",
      "55000/55000 [==============================] - 6s 112us/sample - loss: 0.1451 - val_loss: 0.1470\n",
      "Epoch 7/200\n",
      "55000/55000 [==============================] - 6s 113us/sample - loss: 0.1437 - val_loss: 0.1435\n",
      "Epoch 8/200\n",
      "55000/55000 [==============================] - 6s 115us/sample - loss: 0.1425 - val_loss: 0.1418\n",
      "Epoch 9/200\n",
      "55000/55000 [==============================] - 6s 118us/sample - loss: 0.1416 - val_loss: 0.1400\n",
      "Epoch 10/200\n",
      "55000/55000 [==============================] - 6s 114us/sample - loss: 0.1408 - val_loss: 0.1403\n",
      "Epoch 11/200\n",
      "55000/55000 [==============================] - 6s 112us/sample - loss: 0.1398 - val_loss: 0.1391\n",
      "Epoch 12/200\n",
      "55000/55000 [==============================] - 6s 107us/sample - loss: 0.1388 - val_loss: 0.1398\n",
      "Epoch 13/200\n",
      "55000/55000 [==============================] - 6s 113us/sample - loss: 0.1383 - val_loss: 0.1377\n",
      "Epoch 14/200\n",
      "55000/55000 [==============================] - 6s 112us/sample - loss: 0.1380 - val_loss: 0.1393\n",
      "Epoch 15/200\n",
      "55000/55000 [==============================] - 6s 113us/sample - loss: 0.1378 - val_loss: 0.1383\n",
      "Epoch 16/200\n",
      "55000/55000 [==============================] - 6s 112us/sample - loss: 0.1377 - val_loss: 0.1385\n",
      "Epoch 17/200\n",
      "55000/55000 [==============================] - 6s 114us/sample - loss: 0.1378 - val_loss: 0.1387\n",
      "Epoch 18/200\n",
      "55000/55000 [==============================] - 6s 110us/sample - loss: 0.1379 - val_loss: 0.1417\n",
      "Epoch 19/200\n",
      "55000/55000 [==============================] - 6s 115us/sample - loss: 0.1379 - val_loss: 0.1373\n",
      "Epoch 20/200\n",
      "55000/55000 [==============================] - 6s 114us/sample - loss: 0.1379 - val_loss: 0.1401\n",
      "Epoch 21/200\n",
      "55000/55000 [==============================] - 6s 113us/sample - loss: 0.1378 - val_loss: 0.1373\n",
      "Epoch 22/200\n",
      "55000/55000 [==============================] - 6s 112us/sample - loss: 0.1379 - val_loss: 0.1405\n",
      "Epoch 23/200\n",
      "55000/55000 [==============================] - 6s 116us/sample - loss: 0.1381 - val_loss: 0.1406\n",
      "Epoch 24/200\n",
      "55000/55000 [==============================] - 6s 115us/sample - loss: 0.1383 - val_loss: 0.1387\n",
      "Epoch 25/200\n",
      "55000/55000 [==============================] - 7s 118us/sample - loss: 0.1384 - val_loss: 0.1391\n",
      "Epoch 26/200\n",
      "55000/55000 [==============================] - 6s 110us/sample - loss: 0.1384 - val_loss: 0.1399\n",
      "Epoch 27/200\n",
      "55000/55000 [==============================] - 6s 112us/sample - loss: 0.1384 - val_loss: 0.1391\n",
      "Epoch 28/200\n",
      "55000/55000 [==============================] - 6s 111us/sample - loss: 0.1383 - val_loss: 0.1402\n",
      "Epoch 29/200\n",
      "55000/55000 [==============================] - 6s 113us/sample - loss: 0.1384 - val_loss: 0.1404\n",
      "Epoch 30/200\n",
      "55000/55000 [==============================] - 6s 115us/sample - loss: 0.1383 - val_loss: 0.1377\n",
      "Epoch 31/200\n",
      "55000/55000 [==============================] - 6s 115us/sample - loss: 0.1383 - val_loss: 0.1382\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,restore_best_weights=True)\n",
    "\n",
    "history = variational_ae.fit(X_train, X_train, epochs=200, batch_size=32,\n",
    "                             validation_data=(X_valid, X_valid),  callbacks=[early_stopping_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Classifier #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VAE is trained on entire set, and then SVM is trained on the coding of the labelled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codings_mean,_,_ = variational_encoder.predict(X_train_la)\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_class = SVC(kernel=\"rbf\", gamma='scale', random_state=42, C=1000)\n",
    "svc_class.fit(codings_mean, y_train_la)\n",
    "svc_class.score(codings_mean, y_train_la)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "codings_mean_2,_,_ = variational_encoder.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9118"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_class.score(codings_mean_2, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah, far better results when I use the mean of the codings :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about when I use both the mean and the var?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "codings_mean,codings_var,_ = variational_encoder.predict(X_train_la)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.hstack([codings_mean,codings_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_class = SVC(kernel=\"rbf\", gamma='scale', random_state=42, C=1000)\n",
    "svc_class.fit(train, y_train_la)\n",
    "svc_class.score(train, y_train_la)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "codings_mean2,codings_var2,_ = variational_encoder.predict(X_test)\n",
    "test = np.hstack([codings_mean2,codings_var2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'svc_class' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-121afcdea5eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msvc_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_la\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'svc_class' is not defined"
     ]
    }
   ],
   "source": [
    "svc_class.score(train, y_train_la)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codings_mean2,codings_var2,_ = variational_encoder.predict(X_test)\n",
    "test = np.hstack([codings_mean2,codings_var2])\n",
    "\n",
    "svc_class.score(test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score of 0.8968. A bit worse. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about if we use the last hidden layer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_,_,_,last_hidden_layer = variational_encoder.predict(X_train_la)\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_class = SVC(kernel=\"rbf\", gamma='scale', random_state=42, C=1000)\n",
    "svc_class.fit(last_hidden_layer, y_train_la)\n",
    "svc_class.score(last_hidden_layer, y_train_la)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9082"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_,_,_,last_hidden_layer2 = variational_encoder.predict(X_test)\n",
    "svc_class.score(last_hidden_layer2, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, not as good as the means though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
