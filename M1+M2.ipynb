{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "#check os.environ ld_library_path is the same here as when I do it in python via terminal, if I get issues\n",
    "\n",
    "#sometimes I can't select the GPU. In this case, try: https://forums.fast.ai/t/tip-limiting-tensorflow-to-one-gpu/1995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = keras.backend\n",
    "\n",
    "\n",
    "mnist = keras.datasets.mnist\n",
    "(X_train_full, y_train_full), (X_test,y_test) = mnist.load_data()\n",
    "\n",
    "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
    "\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "#1000 labelled points in train set for this example\n",
    "\n",
    "X_train_la = X_train[:1000]\n",
    "X_train_un = X_train[1000:]\n",
    "\n",
    "y_train_la = y_train[:1000]\n",
    "y_train_un = np.empty((X_train_la.shape[0]))\n",
    "\n",
    "\n",
    "y_train_la = y_train_la.reshape(-1,1)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "y_train_la = encoder.fit_transform(y_train_la)\n",
    "\n",
    "y_train_la=y_train_la.toarray()\n",
    "\n",
    "y_valid = y_valid.reshape(-1,1)\n",
    "y_valid = encoder.transform(y_valid)\n",
    "y_valid = y_valid.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M1 # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "class Sampling_M1(keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        mean, log_var = inputs\n",
    "        return K.random_normal(tf.shape(log_var)) * K.exp(log_var/2) + mean\n",
    "\n",
    "codings_size_M1 = 50\n",
    "\n",
    "inputs_M1 = keras.layers.Input(shape=[28, 28])\n",
    "z_M1 = keras.layers.Flatten()(inputs_M1)\n",
    "z_M1 = keras.layers.Dense(600, activation=\"softplus\")(z_M1)\n",
    "z_M1 = keras.layers.Dense(300, activation=\"softplus\")(z_M1)\n",
    "codings_mean_M1 = keras.layers.Dense(codings_size_M1)(z_M1)\n",
    "codings_log_var_M1 = keras.layers.Dense(codings_size_M1)(z_M1)\n",
    "codings_M1 = Sampling_M1()([codings_mean_M1, codings_log_var_M1])\n",
    "variational_encoder_M1 = keras.models.Model(\n",
    "    inputs=[inputs_M1], outputs=[codings_mean_M1, codings_log_var_M1, codings_M1])\n",
    "\n",
    "\n",
    "#decoder\n",
    "decoder_inputs_M1 = keras.layers.Input(shape=[codings_size_M1])\n",
    "x_M1 = keras.layers.Dense(300, activation=\"softplus\")(decoder_inputs_M1)\n",
    "x_M1 = keras.layers.Dense(600, activation=\"softplus\")(x_M1)\n",
    "x_M1= keras.layers.Dense(28 * 28, activation=\"sigmoid\")(x_M1)\n",
    "outputs_M1 = keras.layers.Reshape([28, 28])(x_M1)\n",
    "variational_decoder_M1 = keras.models.Model(inputs=[decoder_inputs_M1], outputs=[outputs_M1])\n",
    "\n",
    "#vae\n",
    "_, _, codings_M1 = variational_encoder_M1(inputs_M1)\n",
    "reconstructions_M1 = variational_decoder_M1(codings_M1)\n",
    "variational_ae_M1 = keras.models.Model(inputs=[inputs_M1], outputs=[reconstructions_M1])\n",
    "\n",
    "latent_loss_M1 = -0.5 * K.sum(\n",
    "    1 + codings_log_var_M1 - K.exp(codings_log_var_M1) - K.square(codings_mean_M1),\n",
    "    axis=-1)\n",
    "variational_ae_M1.add_loss(K.mean(latent_loss_M1) / 784)\n",
    "#question on how loss is computed too..\n",
    "variational_ae_M1.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\n",
    "\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,restore_best_weights=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "55000/55000 [==============================] - 15s 276us/sample - loss: 0.2089 - val_loss: 0.1731\n"
     ]
    }
   ],
   "source": [
    "history = variational_ae_M1.fit(X_train, X_train, epochs=1, batch_size=32,\n",
    "                             validation_data=(X_valid, X_valid),  callbacks=[early_stopping_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt to save the model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = variational_ae_M1.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_objects = {\"Sampling_M1\":Sampling_M1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n"
     ]
    }
   ],
   "source": [
    "variational_ae_M1.save(\"M1_model.h5\",save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "model_Vae = keras.models.load_model(\"M1_model.h5\",custom_objects=custom_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n"
     ]
    }
   ],
   "source": [
    "variational_ae.save(\"M1_model.h5\",save_format=\"tf\") #doesn't work properly as need to deal with the sampling layer.\n",
    "#refer to geron textbook for how to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate the latent representations\n",
    "\n",
    "\n",
    "_,_,X_train_la_encoded = variational_encoder(X_train_la)\n",
    "X_train_la_encoded = X_train_la_encoded.numpy()\n",
    "\n",
    "_,_,X_train_un_encoded = variational_encoder(X_train_un)\n",
    "X_train_un_encoded = X_train_un_encoded.numpy()\n",
    "\n",
    "_,_,X_valid_encoded = variational_encoder(X_valid)\n",
    "X_valid_encoded = X_valid_encoded.numpy()\n",
    "\n",
    "_,_,X_test_encoded = variational_encoder(X_test)\n",
    "X_test_encoded = X_test_encoded.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"X_train_la_encoded.npy\",X_train_la_encoded)\n",
    "np.save(\"X_train_un_encoded.npy\",X_train_un_encoded)\n",
    "np.save(\"X_valid_encoded.npy\",X_valid_encoded)\n",
    "np.save(\"X_test_encoded.npy\",X_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_un_encoded = np.load(\"X_train_un_encoded.npy\")\n",
    "X_train_la_encoded = np.load(\"X_train_la_encoded.npy\")\n",
    "X_valid_encoded = np.load(\"X_valid_encoded.npy\")\n",
    "X_test_encoded = np.load(\"X_test_encoded.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M2 # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to set input and output shapes depending on x and y shapes\n",
    "x_input_shape = 50\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "#reparameterization trick\n",
    "class Sampling(keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        mean, log_var = inputs\n",
    "        return K.random_normal(tf.shape(log_var)) * K.exp(log_var/2) + mean\n",
    "\n",
    "codings_size = 30\n",
    "\n",
    "x_in = keras.layers.Input(shape=[50])\n",
    "f = keras.layers.Flatten()(x_in)\n",
    "z = keras.layers.Dense(40, activation=\"softplus\")(f)\n",
    "#z = keras.layers.Dense(300, activation=\"softplus\")(z)\n",
    "\n",
    "codings_mean = keras.layers.Dense(codings_size)(z)\n",
    "codings_log_var = keras.layers.Dense(codings_size)(z)\n",
    "codings = Sampling()([codings_mean, codings_log_var])\n",
    "\n",
    " \n",
    "\n",
    "variational_encoder = keras.models.Model(\n",
    "    inputs=[x_in], outputs=[codings_mean, codings_log_var, codings])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_classifier = keras.layers.Dense(30, activation=\"selu\")(f)\n",
    "#y_classifier = keras.layers.Dense(100, activation=\"selu\")(y_classifier)\n",
    "y_pred = keras.layers.Dense(10,activation=\"softmax\")(y_classifier) \n",
    "\n",
    "classifier = keras.models.Model(\n",
    "    inputs=[x_in], outputs=[y_pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "latent = keras.layers.Input(shape=[codings_size])\n",
    "y = keras.layers.Input(shape=[10])\n",
    "\n",
    "l_merged = keras.layers.concatenate([latent,y])\n",
    "x = keras.layers.Dense(40, activation=\"softplus\")(l_merged)\n",
    "#x = keras.layers.Dense(600, activation=\"softplus\")(x)\n",
    "x = keras.layers.Dense(50, activation=\"sigmoid\")(x)\n",
    "x_out = keras.layers.Reshape([50])(x)\n",
    "\n",
    "\n",
    "variational_decoder = keras.models.Model(inputs=[latent,y], outputs=[x_out])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labelled vae\n",
    "_,_, codings = variational_encoder(x_in)\n",
    "y_pred = classifier(x_in)\n",
    "reconstructions = variational_decoder([codings,y])\n",
    "label_vae = keras.models.Model(inputs=(x_in,y), outputs=(reconstructions,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unlabelled vae\n",
    "_,_, codings = variational_encoder(x_in)\n",
    "y_pred = classifier(x_in)\n",
    "reconstructions_un = variational_decoder([codings,y_pred])\n",
    "unlabel_vae = keras.models.Model(inputs=x_in, outputs=reconstructions_un)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelled_loss_reconstruction(codings_log_var,codings_mean):\n",
    "    def loss_functions_labelled(x, x_decoded_mean):\n",
    "        x = K.reshape(x,[-1,50])\n",
    "        x_decoded_mean = K.reshape(x_decoded_mean,[-1,50])\n",
    "        xent_loss = 50*keras.losses.binary_crossentropy(x, x_decoded_mean)        \n",
    "        kl_loss = - 0.5 * K.sum(1 + codings_log_var - K.square(codings_mean) - K.exp(codings_log_var), axis=-1)\n",
    "        return K.mean(xent_loss + kl_loss)\n",
    "    return loss_functions_labelled \n",
    "\n",
    "def unlabelled_loss_reconstruction(codings_log_var,codings_mean,y_pred):\n",
    "    def loss_functions_unlabelled(x,x_decoded_mean):\n",
    "        x = K.reshape(x,[-1,50])\n",
    "        x_decoded_mean = K.reshape(x_decoded_mean,[-1,50])\n",
    "        kl_loss = - 0.5 * K.sum(1 + codings_log_var - K.square(codings_mean) - K.exp(codings_log_var), axis=-1)\n",
    "        xent_loss = 50*keras.losses.binary_crossentropy(x, x_decoded_mean)        \n",
    "        entropy = keras.losses.categorical_crossentropy(y_pred,y_pred)\n",
    "        loss = K.mean(kl_loss + xent_loss)\n",
    "        #need to check below. We are summing over y, but we are assuming that the loss term is independent of y\n",
    "        #which is not the case. How to do it though? https://github.com/bjlkeng/sandbox/issues/3\n",
    "        #and how to do it for regression?\n",
    "        return K.mean(K.sum(y_pred*loss,axis=-1)) + K.mean(entropy)\n",
    "    return loss_functions_unlabelled\n",
    "\n",
    "def labelled_cls_loss(y, y_pred,N=1000):\n",
    "    alpha = 0.1*N\n",
    "    cat_xent_loss = keras.losses.categorical_crossentropy(y, y_pred)\n",
    "    return alpha*K.mean(cat_xent_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_vae.compile(loss=[labelled_loss_reconstruction(codings_log_var,codings_mean)\n",
    "                        ,labelled_cls_loss], optimizer=\"rmsprop\", experimental_run_tf_function=False)\n",
    "\n",
    "unlabel_vae.compile(loss=unlabelled_loss_reconstruction(codings_log_var,codings_mean,y_pred),\n",
    "                    optimizer=\"rmsprop\", experimental_run_tf_function=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_5 (Model)                 [(None, 50), (None,  241000      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_7 (Model)                 (None, 50)           228950      model_5[1][2]                    \n",
      "                                                                 input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "model_6 (Model)                 (None, 10)           46410       input_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 516,360\n",
      "Trainable params: 516,360\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "label_vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 5000 samples\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 136.5636 - model_7_loss: 0.6303 - model_6_loss: 133.0965 - val_loss: 81.4951 - val_model_7_loss: -1.4265 - val_model_6_loss: 82.5993\n"
     ]
    }
   ],
   "source": [
    "#for debugging\n",
    "\n",
    "history = label_vae.fit(\n",
    "    [X_train_la_encoded,y_train_la], [X_train_la_encoded,y_train_la], epochs=1, #batch_size=128,\n",
    "    validation_data=([X_valid_encoded,y_valid], [X_valid_encoded,y_valid])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#for debugging\n",
    "\n",
    "history = unlabel_vae.fit(\n",
    "    [X_train_un_encoded], [X_train_un_encoded], epochs=1, #batch_size=128,\n",
    "    validation_data=([X_valid_encoded], [X_valid_encoded])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create batches ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#omer method - just picks things at random so each epoch may not necessarily go through every single point.\n",
    "#but easier implementation for now\n",
    "\n",
    "def create_batch(x_label, y_label, x_unlabel, batch_s=64):\n",
    "    '''\n",
    "    Creates batches of labelled and unlabelled data. The total number of points in both batches is equal to batch_s. \n",
    "    \n",
    "    '''\n",
    "    proportion_labelled = x_label.shape[0]/(x_label.shape[0] + x_unlabel.shape[0])\n",
    "    \n",
    "    shape_label = x_label.shape[0]\n",
    "    label_per_batch = int(np.ceil(proportion_labelled*batch_s))\n",
    "    batch_idx_la = np.random.choice(list(range(shape_label)), label_per_batch)\n",
    "    batch_x_la = (x_label[batch_idx_la, :])\n",
    "    batch_y_la = (y_label[batch_idx_la])\n",
    "    \n",
    "    shape_unlabel = x_unlabel.shape[0]\n",
    "    unlabel_per_batch = batch_s - label_per_batch\n",
    "    batch_idx_un = np.random.choice(list(range(shape_unlabel)), unlabel_per_batch)\n",
    "    batch_x_un = (x_unlabel[batch_idx_un, :])\n",
    "    \n",
    "    del batch_idx_la,batch_idx_un\n",
    "            \n",
    "    return batch_x_la, batch_y_la, batch_x_un"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progress_bar(iteration, total, size=30):\n",
    "    running = iteration < total\n",
    "    c = \">\" if running else \"=\"\n",
    "    p = (size - 1) * iteration // total\n",
    "    fmt = \"{{:-{}d}}/{{}} [{{}}]\".format(len(str(total)))\n",
    "    params = [iteration, total, \"=\" * p + c + \".\" * (size - p - 1)]\n",
    "    return fmt.format(*params)\n",
    "\n",
    "def print_status_bar(iteration, total, loss, metrics=None, size=30):\n",
    "    metrics = \" - \".join([\"Loss: {:.4f}\".format(loss)])\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{} - {}\".format(progress_bar(iteration, total), metrics), end=end)\n",
    "    \n",
    "def print_status_bar_epoch(iteration, total, loss, validation_loss, metrics=None, size=30):\n",
    "    metrics = \" - \".join([\"Loss: {:.4f} Validation loss: {:.4f} \".format(loss,validation_loss)])\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{} - {}\".format(progress_bar(iteration, total), metrics), end=end)\n",
    "    \n",
    "#could make these functions into just one which works for both if I want. See the Geron textbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(X_train_la, y_train_la, X_train_un,epochs,X_valid_la, y_valid_la,patience,batch_size=64):\n",
    "\n",
    "    \"\"\"\n",
    "    Fits the model. Gets the validation loss too. And includes early stopping, given by the patience.\n",
    "    \n",
    "    \"\"\"\n",
    "    #The callback still doesn't take the model back to the model which performed best.. how to implement that?\n",
    "    #maybe save the model after each epoch in a list of max size 10.\n",
    "    #if we callback is reached, look at val loss list and see index which is smallest and choose that for model restoration\n",
    "    \n",
    "    start = time.time()\n",
    "    history = []\n",
    "    \n",
    "    validation_loss = []\n",
    "    \n",
    "    batches_per_epoch = int(np.floor((X_train_la.shape[0] + X_train_un.shape[0])/batch_size))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "            \n",
    "            print(\"Epoch {}/{}\".format(epoch,epochs))\n",
    "            \n",
    "            for i in range(batches_per_epoch):\n",
    "\n",
    "                batch_x_la, batch_y_la, batch_x_un = create_batch(X_train_la,y_train_la,X_train_un,batch_size)\n",
    "\n",
    "                loss = unlabel_vae.train_on_batch(batch_x_un,batch_x_un)\n",
    "\n",
    "                loss += label_vae.train_on_batch([batch_x_la,batch_y_la],\n",
    "                                                [batch_x_la,batch_y_la])[0] #selecting the overall loss term\n",
    "                \n",
    "\n",
    "                history.append(loss)\n",
    "                print_status_bar(i*batch_size,X_train_la.shape[0] + X_train_un.shape[0],loss)\n",
    "                \n",
    "            val_loss = unlabel_vae.evaluate(X_train_un,X_train_un,verbose=0);\n",
    "            \n",
    "            val_loss += label_vae.evaluate([X_train_la,y_train_la],[X_train_la,y_train_la],verbose=0)[0];\n",
    "            \n",
    "            validation_loss.append(val_loss)\n",
    "                \n",
    "            print_status_bar_epoch(X_train_la.shape[0] + X_train_un.shape[0]\n",
    "                             ,(X_train_la.shape[0] + X_train_un.shape[0]),loss,val_loss)\n",
    "            \n",
    "            #callback for early stopping\n",
    "            if epoch > patience - 1:\n",
    "                \n",
    "                latest_val_loss = validation_loss[-patience:]\n",
    "                if all(i<=val_loss for i in latest_val_loss) is True:\n",
    "                    break\n",
    "            \n",
    "            \n",
    "                \n",
    "    done = time.time()\n",
    "    elapsed = done-start\n",
    "    print(\"Elapsed: \",elapsed)\n",
    "    print(\"Final training loss: \",loss)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/10\n",
      "55000/55000 [==============================] - Loss: -34.9923 Validation loss: 257.3728 \n",
      "Epoch 1/10\n",
      "55000/55000 [==============================] - Loss: -149.3890 Validation loss: 41.8777 \n",
      "Epoch 2/10\n",
      "55000/55000 [==============================] - Loss: -317.7199 Validation loss: -87.4285 \n",
      "Epoch 3/10\n",
      "55000/55000 [==============================] - Loss: -372.6899 Validation loss: -135.5260 \n",
      "Epoch 4/10\n",
      "55000/55000 [==============================] - Loss: -152.3757 Validation loss: -181.5146 \n",
      "Epoch 5/10\n",
      "55000/55000 [==============================] - Loss: -189.3367 Validation loss: -207.4836 \n",
      "Epoch 6/10\n",
      "55000/55000 [==============================] - Loss: -334.2572 Validation loss: -245.6570 \n",
      "Epoch 7/10\n",
      "55000/55000 [==============================] - Loss: -251.5693 Validation loss: -275.2056 \n",
      "Epoch 8/10\n",
      "55000/55000 [==============================] - Loss: -248.8307 Validation loss: -288.2636 \n",
      "Epoch 9/10\n",
      "55000/55000 [==============================] - Loss: -308.4933 Validation loss: -294.7424 \n",
      "Elapsed:  248.06689143180847\n",
      "Final training loss:  -308.49332\n"
     ]
    }
   ],
   "source": [
    "history = fit_model(X_train_la_encoded, y_train_la, X_train_un_encoded,10,X_valid_encoded,y_valid,patience=10,batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 50)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.reshape(-1,1)\n",
    "y_test = encoder.transform(y_test)\n",
    "y_test = y_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction = classifier.predict(X_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "xent_loss = log_loss(y_test,y_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.443769420192088"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xent_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rounded = np.round(y_prediction,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4218"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score as accuracy\n",
    "\n",
    "accuracy_score = accuracy(y_test,y_pred_rounded)\n",
    "\n",
    "accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are very poor. I could have run it for longer. When I did run a different set up for longer, accuracy was just 0.6. Maybe because of how I configured the M2 model. M1 gives it a sample with 50 features, and then M2's coding layer is also 50 features. We want to decrease the latent dimensionality. Also the number of nodes are maybe too big. \n",
    "\n",
    "In the run shown, I've configured it differently, but still poor results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe the models need to be trained jointly, as in having a probabilistic model overall, unlike what is done here, where M1 is used as a feature extractor? Can try this out..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save(\"m1+m2_poor_classifier.h5\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
