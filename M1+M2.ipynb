{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "#check os.environ ld_library_path is the same here as when I do it in python via terminal, if I get issues\n",
    "\n",
    "#sometimes I can't select the GPU. In this case, try: https://forums.fast.ai/t/tip-limiting-tensorflow-to-one-gpu/1995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = keras.backend\n",
    "\n",
    "\n",
    "mnist = keras.datasets.mnist\n",
    "(X_train_full, y_train_full), (X_test,y_test) = mnist.load_data()\n",
    "\n",
    "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
    "\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "#1000 labelled points in train set for this example\n",
    "\n",
    "X_train_la = X_train[:1000]\n",
    "X_train_un = X_train[1000:]\n",
    "\n",
    "y_train_la = y_train[:1000]\n",
    "y_train_un = np.empty((X_train_la.shape[0]))\n",
    "\n",
    "\n",
    "y_train_la = y_train_la.reshape(-1,1)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "y_train_la = encoder.fit_transform(y_train_la)\n",
    "\n",
    "y_train_la=y_train_la.toarray()\n",
    "\n",
    "y_valid = y_valid.reshape(-1,1)\n",
    "y_valid = encoder.transform(y_valid)\n",
    "y_valid = y_valid.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yes this cell is very similar to the above. It's just so I could get a non-one hot encoded y for the SVM task. Probably\n",
    "#should clean this up!\n",
    "\n",
    "(X_train_full, y_train_full), (X_test,y_test2) = mnist.load_data()\n",
    "\n",
    "y_valid2, y_train2 = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "\n",
    "#1000 labelled points in train set for this example\n",
    "\n",
    "\n",
    "y_train_la2 = y_train2[:1000]\n",
    "\n",
    "\n",
    "y_train_la2 = y_train_la2.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M1 # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "class Sampling_M1(keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        mean, log_var = inputs\n",
    "        return K.random_normal(tf.shape(log_var)) * K.exp(log_var/2) + mean\n",
    "\n",
    "codings_size_M1 = 50\n",
    "\n",
    "inputs_M1 = keras.layers.Input(shape=[28, 28])\n",
    "z_M1 = keras.layers.Flatten()(inputs_M1)\n",
    "z_M1 = keras.layers.Dense(600, activation=\"softplus\")(z_M1)\n",
    "z_M1 = keras.layers.Dense(300, activation=\"softplus\")(z_M1)\n",
    "codings_mean_M1 = keras.layers.Dense(codings_size_M1)(z_M1)\n",
    "codings_log_var_M1 = keras.layers.Dense(codings_size_M1)(z_M1)\n",
    "codings_M1 = Sampling_M1()([codings_mean_M1, codings_log_var_M1])\n",
    "variational_encoder_M1 = keras.models.Model(\n",
    "    inputs=[inputs_M1], outputs=[codings_mean_M1, codings_log_var_M1, codings_M1])\n",
    "\n",
    "\n",
    "#decoder\n",
    "decoder_inputs_M1 = keras.layers.Input(shape=[codings_size_M1])\n",
    "x_M1 = keras.layers.Dense(300, activation=\"softplus\")(decoder_inputs_M1)\n",
    "x_M1 = keras.layers.Dense(600, activation=\"softplus\")(x_M1)\n",
    "x_M1= keras.layers.Dense(28 * 28, activation=\"sigmoid\")(x_M1)\n",
    "outputs_M1 = keras.layers.Reshape([28, 28])(x_M1)\n",
    "variational_decoder_M1 = keras.models.Model(inputs=[decoder_inputs_M1], outputs=[outputs_M1])\n",
    "\n",
    "#vae\n",
    "_, _, codings_M1 = variational_encoder_M1(inputs_M1)\n",
    "reconstructions_M1 = variational_decoder_M1(codings_M1)\n",
    "variational_ae_M1 = keras.models.Model(inputs=[inputs_M1], outputs=[reconstructions_M1])\n",
    "\n",
    "beta = 1\n",
    "latent_loss_M1 = beta*-0.5 * K.sum(\n",
    "    1 + codings_log_var_M1 - K.exp(codings_log_var_M1) - K.square(codings_mean_M1),\n",
    "    axis=-1)\n",
    "variational_ae_M1.add_loss(K.mean(latent_loss_M1) / 784)\n",
    "#question on how loss is computed too..\n",
    "variational_ae_M1.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\n",
    "\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,restore_best_weights=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "55000/55000 [==============================] - 4s 76us/sample - loss: 0.2265 - val_loss: 0.1895\n",
      "Epoch 2/100\n",
      "55000/55000 [==============================] - 3s 55us/sample - loss: 0.1784 - val_loss: 0.1702\n",
      "Epoch 3/100\n",
      "55000/55000 [==============================] - 3s 57us/sample - loss: 0.1621 - val_loss: 0.1585\n",
      "Epoch 4/100\n",
      "55000/55000 [==============================] - 3s 58us/sample - loss: 0.1542 - val_loss: 0.1529\n",
      "Epoch 5/100\n",
      "55000/55000 [==============================] - 3s 57us/sample - loss: 0.1491 - val_loss: 0.1463\n",
      "Epoch 6/100\n",
      "55000/55000 [==============================] - 3s 58us/sample - loss: 0.1457 - val_loss: 0.1454\n",
      "Epoch 7/100\n",
      "55000/55000 [==============================] - 3s 59us/sample - loss: 0.1430 - val_loss: 0.1442\n",
      "Epoch 8/100\n",
      "55000/55000 [==============================] - 3s 60us/sample - loss: 0.1409 - val_loss: 0.1417\n",
      "Epoch 9/100\n",
      "55000/55000 [==============================] - 3s 56us/sample - loss: 0.1395 - val_loss: 0.1389\n",
      "Epoch 10/100\n",
      "55000/55000 [==============================] - 3s 55us/sample - loss: 0.1386 - val_loss: 0.1384\n",
      "Epoch 11/100\n",
      "55000/55000 [==============================] - 3s 55us/sample - loss: 0.1378 - val_loss: 0.1380\n",
      "Epoch 12/100\n",
      "55000/55000 [==============================] - 3s 55us/sample - loss: 0.1372 - val_loss: 0.1376\n",
      "Epoch 13/100\n",
      "55000/55000 [==============================] - 3s 58us/sample - loss: 0.1365 - val_loss: 0.1376\n",
      "Epoch 14/100\n",
      "55000/55000 [==============================] - 3s 63us/sample - loss: 0.1361 - val_loss: 0.1391\n",
      "Epoch 15/100\n",
      "55000/55000 [==============================] - 3s 58us/sample - loss: 0.1357 - val_loss: 0.1363\n",
      "Epoch 16/100\n",
      "55000/55000 [==============================] - 3s 56us/sample - loss: 0.1353 - val_loss: 0.1363\n",
      "Epoch 17/100\n",
      "55000/55000 [==============================] - 3s 55us/sample - loss: 0.1349 - val_loss: 0.1353\n",
      "Epoch 18/100\n",
      "55000/55000 [==============================] - 3s 57us/sample - loss: 0.1345 - val_loss: 0.1368\n",
      "Epoch 19/100\n",
      "55000/55000 [==============================] - 3s 55us/sample - loss: 0.1343 - val_loss: 0.1350\n",
      "Epoch 20/100\n",
      "55000/55000 [==============================] - 3s 57us/sample - loss: 0.1340 - val_loss: 0.1358\n",
      "Epoch 21/100\n",
      "55000/55000 [==============================] - 3s 56us/sample - loss: 0.1338 - val_loss: 0.1350\n",
      "Epoch 22/100\n",
      "55000/55000 [==============================] - 3s 56us/sample - loss: 0.1336 - val_loss: 0.1349\n",
      "Epoch 23/100\n",
      "55000/55000 [==============================] - 3s 59us/sample - loss: 0.1333 - val_loss: 0.1347\n",
      "Epoch 24/100\n",
      "55000/55000 [==============================] - 3s 56us/sample - loss: 0.1332 - val_loss: 0.1358\n",
      "Epoch 25/100\n",
      "55000/55000 [==============================] - 3s 55us/sample - loss: 0.1330 - val_loss: 0.1351\n",
      "Epoch 26/100\n",
      "55000/55000 [==============================] - 3s 58us/sample - loss: 0.1329 - val_loss: 0.1340\n",
      "Epoch 27/100\n",
      "55000/55000 [==============================] - 3s 57us/sample - loss: 0.1327 - val_loss: 0.1348\n",
      "Epoch 28/100\n",
      "55000/55000 [==============================] - 3s 57us/sample - loss: 0.1326 - val_loss: 0.1339\n",
      "Epoch 29/100\n",
      "55000/55000 [==============================] - 3s 55us/sample - loss: 0.1325 - val_loss: 0.1346\n",
      "Epoch 30/100\n",
      "55000/55000 [==============================] - 3s 56us/sample - loss: 0.1324 - val_loss: 0.1347\n",
      "Epoch 31/100\n",
      "55000/55000 [==============================] - 3s 56us/sample - loss: 0.1323 - val_loss: 0.1342\n",
      "Epoch 32/100\n",
      "55000/55000 [==============================] - 3s 56us/sample - loss: 0.1322 - val_loss: 0.1344\n",
      "Epoch 33/100\n",
      "55000/55000 [==============================] - 3s 58us/sample - loss: 0.1321 - val_loss: 0.1340\n",
      "Epoch 34/100\n",
      "55000/55000 [==============================] - 3s 56us/sample - loss: 0.1321 - val_loss: 0.1344\n",
      "Epoch 35/100\n",
      "55000/55000 [==============================] - 3s 59us/sample - loss: 0.1320 - val_loss: 0.1349\n",
      "Epoch 36/100\n",
      "55000/55000 [==============================] - 3s 58us/sample - loss: 0.1320 - val_loss: 0.1347\n",
      "Epoch 37/100\n",
      "55000/55000 [==============================] - 3s 58us/sample - loss: 0.1319 - val_loss: 0.1338\n",
      "Epoch 38/100\n",
      "55000/55000 [==============================] - 3s 58us/sample - loss: 0.1319 - val_loss: 0.1343\n",
      "Epoch 39/100\n",
      "55000/55000 [==============================] - 3s 58us/sample - loss: 0.1318 - val_loss: 0.1339\n",
      "Epoch 40/100\n",
      "55000/55000 [==============================] - 3s 59us/sample - loss: 0.1319 - val_loss: 0.1345\n",
      "Epoch 41/100\n",
      "55000/55000 [==============================] - 3s 55us/sample - loss: 0.1319 - val_loss: 0.1338\n",
      "Epoch 42/100\n",
      "55000/55000 [==============================] - 3s 58us/sample - loss: 0.1319 - val_loss: 0.1343\n",
      "Epoch 43/100\n",
      "55000/55000 [==============================] - 3s 59us/sample - loss: 0.1318 - val_loss: 0.1343\n",
      "Epoch 44/100\n",
      "55000/55000 [==============================] - 3s 57us/sample - loss: 0.1319 - val_loss: 0.1355\n",
      "Epoch 45/100\n",
      "55000/55000 [==============================] - 3s 56us/sample - loss: 0.1319 - val_loss: 0.1333\n",
      "Epoch 46/100\n",
      "55000/55000 [==============================] - 3s 56us/sample - loss: 0.1319 - val_loss: 0.1337\n",
      "Epoch 47/100\n",
      "55000/55000 [==============================] - 3s 56us/sample - loss: 0.1319 - val_loss: 0.1342\n",
      "Epoch 48/100\n",
      "55000/55000 [==============================] - 3s 58us/sample - loss: 0.1319 - val_loss: 0.1337\n",
      "Epoch 49/100\n",
      "55000/55000 [==============================] - 3s 55us/sample - loss: 0.1319 - val_loss: 0.1355\n",
      "Epoch 50/100\n",
      "55000/55000 [==============================] - 3s 58us/sample - loss: 0.1320 - val_loss: 0.1334\n",
      "Epoch 51/100\n",
      "55000/55000 [==============================] - 3s 57us/sample - loss: 0.1321 - val_loss: 0.1359\n",
      "Epoch 52/100\n",
      "55000/55000 [==============================] - 3s 60us/sample - loss: 0.1321 - val_loss: 0.1342\n",
      "Epoch 53/100\n",
      "55000/55000 [==============================] - 3s 58us/sample - loss: 0.1321 - val_loss: 0.1364\n",
      "Epoch 54/100\n",
      "55000/55000 [==============================] - 3s 55us/sample - loss: 0.1322 - val_loss: 0.1356\n",
      "Epoch 55/100\n",
      "55000/55000 [==============================] - 3s 58us/sample - loss: 0.1323 - val_loss: 0.1343\n"
     ]
    }
   ],
   "source": [
    "history = variational_ae_M1.fit(X_train, X_train, epochs=100, batch_size=64,\n",
    "                             validation_data=(X_valid, X_valid),  callbacks=[early_stopping_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt to save the model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = variational_ae_M1.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_objects = {\"Sampling_M1\":Sampling_M1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n"
     ]
    }
   ],
   "source": [
    "variational_ae_M1.save(\"M1_model.h5\",save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "model_Vae = keras.models.load_model(\"M1_model.h5\",custom_objects=custom_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n"
     ]
    }
   ],
   "source": [
    "variational_ae.save(\"M1_model.h5\",save_format=\"tf\") #doesn't work properly as need to deal with the sampling layer.\n",
    "#refer to geron textbook for how to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer flatten is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Generate the latent representations\n",
    "\n",
    "\n",
    "X_train_la_encoded,_,_ = variational_encoder_M1(X_train_la)\n",
    "X_train_la_encoded = X_train_la_encoded.numpy()\n",
    "\n",
    "X_train_un_encoded,_,_ = variational_encoder_M1(X_train_un)\n",
    "X_train_un_encoded = X_train_un_encoded.numpy()\n",
    "\n",
    "X_valid_encoded,_,_ = variational_encoder_M1(X_valid)\n",
    "X_valid_encoded = X_valid_encoded.numpy()\n",
    "\n",
    "X_test_encoded,_,_ = variational_encoder_M1(X_test)\n",
    "X_test_encoded = X_test_encoded.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"X_train_la_encoded.npy\",X_train_la_encoded)\n",
    "np.save(\"X_train_un_encoded.npy\",X_train_un_encoded)\n",
    "np.save(\"X_valid_encoded.npy\",X_valid_encoded)\n",
    "np.save(\"X_test_encoded.npy\",X_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_un_encoded = np.load(\"X_train_un_encoded.npy\")\n",
    "X_train_la_encoded = np.load(\"X_train_la_encoded.npy\")\n",
    "X_valid_encoded = np.load(\"X_valid_encoded.npy\")\n",
    "X_test_encoded = np.load(\"X_test_encoded.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seaborn.distplot(X_test_encoded)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "seaborn.distplot(X_valid_encoded)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rp542/venv/lib/python3.5/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.979"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_class = SVC(kernel=\"rbf\", gamma='scale', random_state=42)\n",
    "svc_class.fit(X_train_la_encoded_scaled, y_train_la2)\n",
    "svc_class.score(X_train_la_encoded_scaled, y_train_la2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.925"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_class.score(X_test_encoded_scaled, y_test2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9312"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_class.score(X_valid_encoded_scaled, y_valid2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've got a useful latent rep in terms of doing classification. Great. Hopefully this results in good scores downstream in M2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_un_encoded_scaled = scaler.fit_transform(X_train_un_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_la_encoded_scaled = scaler.transform(X_train_la_encoded)\n",
    "X_valid_encoded_scaled = scaler.transform(X_valid_encoded)\n",
    "X_test_encoded_scaled = scaler.transform(X_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler2= MinMaxScaler()\n",
    "\n",
    "X_train_un_encoded_scaled = scaler2.fit_transform(X_train_un_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_la_encoded_scaled = scaler2.transform(X_train_la_encoded)\n",
    "X_valid_encoded_scaled = scaler2.transform(X_valid_encoded)\n",
    "X_test_encoded_scaled = scaler2.transform(X_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3XtwXOd53/Hvszfs4g4CIAjeRFKkJUtudTGrm91YlWtX9mQke2Knsjqx3VGqOrFrZ+ppx046SqrMtJNmJu048qXyZSK7oWVHTh3Kpu3YkVTfJEiwQomiLhYoUiRB3Ik7doEF9u0fuwtBEIBdYM/u2cvvM7PDxe7h7nMGwA/vvuc57zHnHCIiUl0CfhcgIiLeU7iLiFQhhbuISBVSuIuIVCGFu4hIFVK4i4hUIYW7iEgVUriLiFQhhbuISBUK+fXGHR0dbt++fX69vYhIRfrVr3416pzrzLWdb+G+b98+ent7/Xp7EZGKZGav5rOdpmVERKqQwl1EpAop3EVEqpDCXUSkCincRUSqkMJdRKQKKdxFRKqQwl1EpAop3EV8EF9Y4uY/f5RPPfiPTCWSfpcjVUjhLuKDx14a5szYHH93/AK/+bmf88y5Cb9LkiqjcBfxwRceO0VDJMi/++cHmIonef8XfsF/+b/P+V2WVBGFu0iJxReWeHFwiit3trC/o4H/cMshWmJhnjpz0e/SpIoo3EVK7LGXhkkuOf7J7hYAYpEgB7c3cWZslqWU87k6qRYKd5ES+/6JARoiQfa1Nyw/tr+jnvnFFC8MTPlYmVQThbtICcUXlviHF4a5cmcLwYAtP54N+p7TmpoRbyjcRUrosZeGiSeXlqdkslrrI7TVh3ny9JhPlUm1UbiLlND3TwzQ3hB53ZRM1v6OBp48fRHnNO8uhVO4i5RIIpmekvlXb9nxuimZrP0dDYzPJekbnvGhOqk2CneREukbniGeXOLtBzvWfD47mn9C8+7iAYW7SIm8PDwNwJu6Gtd8fltDhK7mOp5UuIsHFO4iJfLy0AyhgHHJGvPtAGbG9fvbefL0mObdpWAKd5ESeXl4hv0dDYSD6//aXbd/G0NT85y9OFfCyqQaKdxFSqRveIZD60zJZF2/fxsAPa9oakYKo3AXKYFEcolXx2Y5uL1pw+0Obm9kW0OE3lcV7lKYnOFuZlEze9LMnjGzk2b2X9fYps7MvmVmfWbWY2b7ilGsSKU6PTpLysGh7RuP3M2MQ9sbOTUyW6LKpFrlM3KfB25xzl0FXA3camY3rNrmLmDcOXcQ+J/An3lbpkhleznTu55rWgbS/e5nRhXuUpic4e7SsmdVhDO31YfybwceyNx/CHinmb3xLA2RGtU3NE3A0sGdy76OBsZmF3SFJilIXnPuZhY0s+PAMPBj51zPqk12AecAnHOLwCTQvsbr3G1mvWbWOzIyUljlIhXk5eEZLmlvoC4UzLlt9mQmjd6lEHmFu3NuyTl3NbAbuM7M3rKVN3PO3e+cO+ycO9zZ2bmVlxCpSC8Pz3Awx3x71r6OeiA9Ty+yVaHNbOycmzCzR4FbgZXXBOsH9gDnzSwEtABa3k4EWFhMcWZ0lndf0ZVz2yM9Z1lYTAFw9JkLzM4vAXDn9XuLWqNUn3y6ZTrNrDVzPwa8C3hx1WZHgY9k7n8AeMTpFDsRAF4dm2Ux5fI6mAoQCQVoiYW5OLNQ5MqkmuUzcu8GHjCzIOk/Bt92zn3PzO4Fep1zR4GvAt8wsz7gInBH0SoWqSBHes5yon8SgL7hWY70nM3r/7U3RBidmS9maVLlcoa7c+5Z4Jo1Hr9nxf0E8EFvSxOpDsPTCQzobKzL+/+0N9Zx8sJk8YqSqqczVEWKbGR6ntb6MJFQ/r9uHY0R5haWiC8sFbEyqWYKd5EiG56aZ3tTdFP/p70hPcrX1IxslcJdpIiWUo7RmXm2N+U/JQPQ3hgBYGxW4S5bo3AXKaLxuQUWU47OTYb7toYIBoyqY0a2SOEuUkTjc+lw3pYZiecrHAzQUh9mTNMyskUKd5EimphLrw/TFttcuAN0NNQxNquRu2yNwl2kiCbmFjCgORbe9P9tb0z3uut8QNkKhbtIEU3MJWmOhQkGNr9IantjHYlkijm1Q8oWKNxFimginqR1C6N2gI6GTMeM5t1lCxTuIkU0MbdAa/3Wwr09c0brqObdZQsU7iJFspRyTMaTtNZv/mAqQFtDGAPG1A4pW6BwFymSoakEKceWR+6hQIDW+rBOZJItUbiLFMmFiTgAbVscuQO0NUQY17SMbIHCXaRI+jPh3rLFA6oArbEwk3FdS1U2T+EuUiTnxwsfubfEIkwnFllcSnlVltQIhbtIkfRPxKmPBDe11O9qrbEwDhia1ry7bI7CXaRILkzEt3wwNasl8/8HMlM8IvlSuIsUSf94nNYtrCmzUna+/sJkwouSpIYo3EWKwDlHvxcj95hG7rI1CneRIpiYSzK3sLTlE5iyouEgdaEAAxq5yyYp3EWKINsGudV1ZVZqiYWXe+ZF8qVwFymCfg9OYMpqrQ9r5C6bljPczWyPmT1qZs+b2Ukz+9Qa29xsZpNmdjxzu6c45YpUhv5Mj3tLgXPukB65D0xq5C6bE8pjm0Xg0865p82sCfiVmf3YOff8qu1+5pz7Te9LFKk8FybiRMMBGiLBgl+rJRZmdGaB+cUl6kKFv57Uhpwjd+fcgHPu6cz9aeAFYFexCxOpZP0TcXa1xjDb/EU6VmvJtFMOampGNmFTc+5mtg+4BuhZ4+kbzewZM/uBmV3pQW0iFat/Is6utnpPXmu5131C4S75yzvczawR+A7wB865qVVPPw1c4py7CvhL4LvrvMbdZtZrZr0jIyNbrVmk7PWPp0fuXsh23GjeXTYjr3A3szDpYP9r59zfrn7eOTflnJvJ3D8GhM2sY43t7nfOHXbOHe7s7CywdJHylEguMTa7wK7WqCev17wc7hq5S/7y6ZYx4KvAC865v1hnmx2Z7TCz6zKvO+ZloSKVItsGuavNm5F7JBSgrV697rI5+XTLvA34HeCEmR3PPPaHwF4A59yXgA8Av2dmi0AcuMM554pQr0jZy4bwzpYYp0ZmPXnN7paYRu6yKTnD3Tn3c2DDQ/7OufuA+7wqSqSSDU2ll+fd0RL1LNx3tkaX14cXyYfOUBXx2NBUeoTd1ezNnDto5C6bp3AX8djgZILW+jDRsHcnHHW3RpmMJ5lbWPTsNaW6KdxFPDY4laCrybtRO7DcVqled8mXwl3EY0NTCbpavA337pZ0uKvXXfKlcBfx2NBUgh3NdZ6+Znfmj8WARu6SJ4W7iIcWl1KMTM+zw8ODqZDuvDGDCxq5S54U7iIeGp1ZIOVgu8fhHg4G6Gys08hd8qZwF/HQYKYN0uuRO0B3a0wjd8mbwl3EQ9ke9x0eH1AF6G6OatlfyZvCXcRDxTiBKWtHS3T5k4FILgp3EQ8NTiYIBYz2hsKvnbpaV3OU6cSiTmSSvCjcRTw0OJVge1MdgUDhV2BarSvTXpldu0ZkIwp3EQ8NT817fgJTVvYgrebdJR8KdxEPDU4litIpAyz/0RjSvLvkIZ/13EUkT0OTCd5+8A0XISvYkZ6zzCeXAPjhc4PMLaTv33n9Xs/fS6qDRu4iHpmdX2R6frEonTIAdeEgdaEAk4lkUV5fqotG7iIeONJzlpHp9IHOV0ZmONJztijv0xwNMx1XuEtuGrmLeGQqM6LOXtC6GJpjIaYSaoWU3BTuIh6Zyoyom6NFDPdoePl9RDaicBfxSHZE3Rwr3mxncyzMVCJJSteflxwU7iIemYonqQsFqAt5d3m91ZqjIVIuffBWZCMKdxGPTCWSRZ1vh9fm86c17y455Ax3M9tjZo+a2fNmdtLMPrXGNmZmnzOzPjN71syuLU65IuVrKp6kOVrcBrTsfL7m3SWXfEbui8CnnXNXADcAHzezK1Zt8x7gUOZ2N/BFT6sUqQBTicWiHkyF10bu6nWXXHKGu3NuwDn3dOb+NPACsGvVZrcDX3dpTwCtZtbtebUiZSrlHNMlmJZprAthwFRc0zKysU3NuZvZPuAaoGfVU7uAcyu+Ps8b/wCIVK3Z+UVSrrg97gDBgNEYDS331IusJ+9wN7NG4DvAHzjnprbyZmZ2t5n1mlnvyMjIVl5CpCwtt0EWec49/R5hphXukkNe4W5mYdLB/tfOub9dY5N+YM+Kr3dnHnsd59z9zrnDzrnDnZ2dW6lXpCyV4gSmrOZYWNMyklM+3TIGfBV4wTn3F+tsdhT4cKZr5gZg0jk34GGdImWtFEsPZDVHQ0yqW0ZyyOcz5NuA3wFOmNnxzGN/COwFcM59CTgGvBfoA+aAf+t9qSLlazKeJGDQVIppmViYeHKJ5FKq6O8llSvnT6Jz7ufAhtcMc8454ONeFSVSaabiizTWhQiY95fXW0297pIPnaEq4oGpeJKWEkzJwGtr12h1SNmIwl3EA5Ml6HHPWh65q2NGNqBwF/HAVNyHcNe0jGxA4S5SoOlEkvnFFC0laIMEiIYDhIOmcJcNKdxFCjQ0lQBK0wYJYGbpi3Zozl02oHAXKdDgZPraqcW8SMdq2Yt2iKxH4S5SoMHMyL1U0zIALTFdbk82pnAXKdDgZBwo3bQMZK+lukgqpcvtydoU7iIFGpxKEAsHCQdL9+vUEgux5Bxjswsle0+pLAp3kQINTiZKdgJTVkssAsBA5lODyGoKd5ECDU4lSnowFVj+YzIwmSjp+0rlULiLFMiXkXt9JtwnNHKXtSncRQqwsJhidGahJOu4r1QfCRIMGANTGrnL2hTuIgXInsBU6pF7wIyWWJiBCYW7rE3hLlKAUp+dulJzNMyg5txlHQp3kQJkD2j6Ee6t9WEuqFtG1qFwFynAkA9np2Y1R8MMTSV0IpOsSeEuUoCByQTRcIBouPS/Si31YZJLjtHZ+ZK/t5Q/hbtIAQanEnS3xLASXF5vteynBc27y1oU7iIFGJpM0NVc58t7Z3vdL6hjRtagcBcpwMBkeuTuh2z75aAOqsoaFO4iW5RKOYanE3Q1R315/4ZIkEgwoCUIZE0Kd5EtGptdILnk6G7xJ9zNjB0tUYW7rClnuJvZ18xs2MyeW+f5m81s0syOZ273eF+mSPnJtkH6NXIH6G6JamVIWVM+I/e/Am7Nsc3PnHNXZ273Fl6WSPnLjpj9Grln31sjd1lLznB3zv0UuFiCWkQqSvZA5g4/w701phOZZE1ezbnfaGbPmNkPzOxKj15TpKz1TyQIB43ORn9aISE9cteJTLIWL8L9aeAS59xVwF8C311vQzO728x6zax3ZGTEg7cW8U//RJzulhiBQOlPYMrKtmFqdUhZreBwd85NOedmMvePAWEz61hn2/udc4edc4c7OzsLfWsRX12YiLOr1Z8e96zsfL/m3WW1gsPdzHZY5txrM7su85pjhb6uSLnrH4+zs2zCXR0z8no5L/xoZt8EbgY6zOw88MdAGMA59yXgA8DvmdkiEAfucM7p6I5UteRSiqHpBLva/A33bQ0RIqGA1peRN8gZ7s65D+V4/j7gPs8qEqkAg5MJnINdrf51ykD6RKbuligXFO6yis5QFdmC/syFqf2elgHY0RzV+jLyBgp3kS24kAl3vw+oQvoPjFaGlNUU7iJb0D9ePiP3na1RhqYSLC6l/C5FyojCXWQLLkzG6WiMEA0H/S6F3W31LKYcQ9M6kUleo3AX2YLzZdAGmbU707Fz/uKcz5VIOVG4i2xBOZzAlLW7rR547SCvCOTRCikir+eco38izs2Xbfe7FI70nF2eaz92YoBEMn3/zuv3+lmWlAGN3EU2aXwuSSKZKpuReygYoCkaYnwu6XcpUkY0chfZhCM9Z5c7ZU6NzHCk56zPFaW11UcYn1vwuwwpIxq5i2zSRDwdoq31EZ8reU1rfZgJjdxlBYW7yCZlQ7Q1Fva5kte01UeYmFsgpWWdJEPhLrJJE3MLhINGfcT/HvestvoIKQdTcY3eJU3hLrJJE/EkrbEImZWuy0JbffpThA6qSpbCXWSTJuaStNaXz5QMpEfukP5UIQIKd5FNm4iXX7i3aOQuqyjcRTYhuZRidn6Rllj5dMoAhDO97hq5S5bCXWQTJjMj47YyG7lDuntHve6SpXAX2YTxTI97SxmGe1tDRNMyskzhLrIJyyP3MpuWgfRB1cm5pHrdBVC4i2zKxbkFAgbNZXQCU1ZrfZgl55hOLPpdipQBhbvIJozNLNBaHyEYKJ8e96xsO+T4rObdReEusiljs/O0N5TflAysCHcdVBXyCHcz+5qZDZvZc+s8b2b2OTPrM7Nnzexa78sU8Z9zjrGZBdob6/wuZU2t6nWXFfIZuf8VcOsGz78HOJS53Q18sfCyRMrPxdkF5hdTZTtyDwcDNNWp113Scoa7c+6nwMUNNrkd+LpLewJoNbNurwoUKRdnxmYBaG8sz3AHLf0rr/Fizn0XcG7F1+czj4lUlTOj6QtQdzSU57QMpNeY15y7QIkPqJrZ3WbWa2a9IyMjpXxrkYKdGZvFgNaG8muDzEqv655kKaVe91rnRbj3A3tWfL0789gbOOfud84dds4d7uzs9OCtRUrnzNgcbQ0RQoHybTLraIyw5Bznx+f8LkV85sVP6VHgw5mumRuASefcgAevK1JWzozOlu3B1KzOpvSU0amRGZ8rEb/l0wr5TeBx4DIzO29md5nZx8zsY5lNjgGvAH3Al4HfL1q1Ij5xznFmbLasD6bCinAfnvW5EvFbKNcGzrkP5XjeAR/3rCKRMjQ+l2Q6sUh7GR9MBaiPhGioC2nkLjpDVSQfp0czbZBlPi0D0NlYR9+wwr3WKdxF8vDqco97eY/cIT01o5G7KNxF8nBmdJaAQVsZt0FmdTbVMT6X5KIWEKtpCneRPJwZm2NXW6ys2yCztmcOqmpqpraV/0+qSBl4dWyWfe0NfpeRl85GtUOKwl0kJ+ccp0crJ9xb6sPUhQKc0si9pincRXKYmEsylVjkkvZ6v0vJS8CMA52NGrnXOIW7SA6nM50ylTJyBzi4vZE+hXtNU7iL5JBtg9zXUTnhfmlnA+fH4ySSS36XIj5RuIvkcHp0DjPYsy3mdyl5u7SzEedeO/lKao/CXSSHlwan2N/eQF0o6Hcpebu0sxFQx0wtU7iL5PD8wBRv3tnsdxmbcqCzATP1utcyhbvIBibjSc5djHNFd2WFezQcZHdbjFMjmpapVQp3kQ28ODAFwBUVNnKH9NSMet1rl8JdZAPPZ8L9ygobuQMc7GzkldEZUrrkXk1SuIts4PkLU3Q0RpYvglFJDnU1kkimePWiLrlXi3JerEOklj0/MMWbu5sxM79L2ZQjPWcZmIwD8MXHTnH1nlYA7rx+r59lSQlp5C6yjoXFFC8PzVTkfDvA9qYo4aDRr4tl1ySN3EXWkB35LiylmJhNcqTnrN8lbVowYHS3xDg/Hve7FPGBRu4i6xiYTADQ3RL1uZKt290W48JknCUdVK05CneRdQxMxAkHjY4KPJiatbstRnLJMTyd8LsUKTGFu8g6BiYTdDVHCVTYwdSVdremlynu19RMzVG4i6zBOcfAZILulspZLGwt2xojRMMBzbvXoLzC3cxuNbOXzKzPzD6zxvMfNbMRMzueuf2u96WKlM5kPEk8uVTR8+2QvnDHztYY5yfUMVNrcoa7mQWBzwPvAa4APmRmV6yx6becc1dnbl/xuE6RksoeTN1Z4eEO6amZwckEyaWU36VICeUzcr8O6HPOveKcWwAeBG4vblki/rowGceArmoI97YYKQeDkzqoWkvyCfddwLkVX5/PPLbab5nZs2b2kJntWeuFzOxuM+s1s96RkZEtlCtSGucvxulorKuoNdzXs7stfdzgvE5mqileHVB9GNjnnPunwI+BB9bayDl3v3PusHPucGdnp0dvLeKt5FKK02OzHOisnMvqbaQlFqahLqSDqjUmn3DvB1aOxHdnHlvmnBtzzs1nvvwK8FZvyhMpvRP9kywspjiQuZpRpTMzdrfG6J9QuNeSfML9KeCQme03swhwB3B05QZm1r3iy9uAF7wrUaS0Hj81BsD+Crogdi6722KMTM8zM7/odylSIjnD3Tm3CHwC+BHp0P62c+6kmd1rZrdlNvukmZ00s2eATwIfLVbBIsX2xCtjdDXX0VhXPUsvXdLegAOePD3mdylSInn99DrnjgHHVj12z4r7nwU+621pIqW3sJjiqTMXuWZvm9+leGpfez3hoPHoiyPccnmX3+VICegMVZEVjp+bIJFMcWkVTckAhIIBDnY28siLwzinRcRqgcJdZIXHT41hBvs7quNg6kpv2tFE/0ScPl1XtSYo3EVWePyVUa7obiYWqfz+9tUu62oC4NGXhn2uREpB4S6SkUgu8fTZCW480O53KUXRWh/h8h1NPPqiTiCsBQp3kYynz46zsJjipoPVGe4AN1+2nafOXGQ6kfS7FCkyhbtIxuOnxggGjH+2b5vfpRTNv7isk8WU4xd9o36XIkWmcBchvX77358c4qrdLTRFw36XUzTXXtJGUzTEIy9q3r3aKdxFgH88N8FLQ9N84K1rrnlXNcLBAL9xqJNHXxpRS2SVU7iLAA8+eZb6SJDbrt7pdylFd/NlnYxMz3P83ITfpUgRVc/51SJbNJ1I8vAzA9x21c6qWnJgLUd6zpJILhEJBbj34ef54OH0J5U7r9/rc2Xiter+SRbJ4UjPWXpOjxFPLrGtIcKRnrN+l1R00XCQa/e28dTpi9z6lh1VfYyhlmlaRmpe75lxdjRHly9qUQtuOtDOknP0nL7odylSJAp3qWkXJuL0T8Q5vK8NM/O7nJLpaKrjsq4mek5fZFHXVq1KCnepaU+euUgoYFyzp7pWgczHTQfbmZ1f5Nn+Sb9LkSJQuEvN6hueoffMRa7e01qVa8nkcrCzke1Ndfzy1KjaIquQwl1qknOOPzl6kkgowLuv3OF3Ob4wM268tJ0LEwn+36+13ky1UbhLTTp2YpCf943yrjd3VX3740au3dtGZ2Md//mhZxmbmc/9H6RiKNyl5szOL/Kn33ueK7qbuW5/9S4Slo9wMMAd1+1hYi7Jf3roWU3PVBGFu9Sc//WTXzM4leBP33clwUDtdMisp7slxmffezmPvDjMA78843c54hGFu9SUB355hi//7DQfum4Pb72keld/3KyP3rSPWy7fzn879iK9Z9T7Xg0U7lIzvvHEq/zx0ZO8+4ou7r39LX6XU1a++eQ5bjjQTlM0xB33P8FnvvNsTZytW81q90iS1AznHJ988DgPP3OBy3c08fZDHfxN73m/yyo7jXUhPvaOS/k/Pa/y4FPnuDi7wIeu21NTJ3dVk7xG7mZ2q5m9ZGZ9ZvaZNZ6vM7NvZZ7vMbN9XhcqshXPnp/gt//34zz8zAUu62rizuv2EgroA+t6GupC3PW2/Vy1u4W/f36IO7/co9UjK1TOkbuZBYHPA+8CzgNPmdlR59zzKza7Cxh3zh00szuAPwP+dTEKFsllfHaBn/WN8qPnBvn+iQE6GiO8/+pdvHVfGwGNQnMKBQP89uE97G1v4Jd9o7zv87/g1it38G9u2MtbL2mjPqIP/JUgn+/SdUCfc+4VADN7ELgdWBnutwN/krn/EHCfmZlTX5V4xDnHUsqxsJRiYTFFIpliOpFkMp5kfC7JKyMzvDw8wy9PjTIwkcABsXCQd7ypk3e8qZNouPbOQC2EmXHjgXau3dPKz0+N8uhLw/zw5CABg91t9dx8WSd72urZs62ezqYI9ZEQDZEQ0UiAaDhINBQkHDRN6fgon3DfBZxb8fV54Pr1tnHOLZrZJNAOeH6hxh8+N8inv33c65cVD23lL/rKYYDDkXLpF0o5x5Jz5DNM2N5UR1M0xC2Xb+dNXU3saotppF6gunCQd17exdsPdnB2bI5XRmc5PTrLt3vPkUjmXnAsYBAwI2CGGekb6ftryT68lZ+hSvpO3/X2/fzHd19W1Pco6ecrM7sbuDvz5YyZvVTK91+hgyL84SkT1bpvOffr1RIVUgTV+j2D6t23gvbr05nbFl2Sz0b5hHs/sPLCkrszj621zXkzCwEtwNjqF3LO3Q/cn09hxWRmvc65w37XUQzVum/Vul+gfatElbBf+bQNPAUcMrP9ZhYB7gCOrtrmKPCRzP0PAI9ovl1ExD85R+6ZOfRPAD8CgsDXnHMnzexeoNc5dxT4KvANM+sDLpL+AyAiIj7Ja87dOXcMOLbqsXtW3E8AH/S2tKLyfWqoiKp136p1v0D7VonKfr9MsyciItVHp+qJiFShmgh3M/ugmZ00s5SZrXuEO9cyC+XIzLaZ2Y/N7OXMv2teDNTMlszseOa2+oB42ajmpS7y2LePmtnIiu/T7/pR52aZ2dfMbNjMnlvneTOzz2X2+1kzu7bUNW5FHvt1s5lNrvh+3bPWdr5xzlX9DXgzcBnwGHB4nW2CwCngABABngGu8Lv2PPbtfwCfydz/DPBn62w343eteexLzu8B8PvAlzL37wC+5XfdHu7bR4H7/K51C/v2G8C1wHPrPP9e4AekzzO6Aejxu2aP9utm4Ht+17nerSZG7s65F5xzuU6YWl5mwTm3AGSXWSh3twMPZO4/ALzPx1oKlc/3YOX+PgS80yrjHPdK/fnKyTn3U9Jdcuu5Hfi6S3sCaDWz7tJUt3V57FdZq4lwz9Nayyzs8qmWzehyzg1k7g8CXetsFzWzXjN7wszK9Q9APt+D1y11AWSXuih3+f58/VZm6uIhM9uzxvOVqFJ/t/Jxo5k9Y2Y/MLMr/S5mpapZ3s3MfgKsdRn7P3LO/V2p6/HSRvu28gvnnDOz9dqfLnHO9ZvZAeARMzvhnDvlda1SkIeBbzrn5s3s35P+hHKLzzXJ+p4m/Xs1Y2bvBb4LHPK5pmVVE+7OuX9Z4Evks8yCLzbaNzMbMrNu59xA5qPu8Dqv0Z/59xUzewy4hvQccDnxbKmLMpRz35xzK/fjK6SPp1SDsv3dKoRzbmrF/WNm9gUz63DOlcVaOpqWeU0+yyyUo5VLP3wEeMOnFDNrM7O6zP0O4G28fsnmclHNS13k3LdV89C3AS+UsL5iOgp8ONM1cwMwuWJy2xenAAAA4ElEQVQqsWKZ2Y7s8R4zu450npbPQMPvI7qluAHvJz3PNw8MAT/KPL4TOLZiu/cCvyY9ov0jv+vOc9/agX8AXgZ+AmzLPH4Y+Erm/k3ACdIdGieAu/yue4P9ecP3ALgXuC1zPwr8DdAHPAkc8LtmD/ftvwMnM9+nR4HL/a45z/36JjAAJDO/Z3cBHwM+lnneSF/w51Tm52/NjrVyu+WxX59Y8f16ArjJ75pX3nSGqohIFdK0jIhIFVK4i4hUIYW7iEgVUriLiFQhhbuISBVSuIuIVCGFu4hIFVK4i4hUof8PSh8nVRXhH+AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "seaborn.distplot(X_test_encoded_scaled)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M2 # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to set input and output shapes depending on x and y shapes\n",
    "x_input_shape = 50\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "#reparameterization trick\n",
    "class Sampling(keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        mean, log_var = inputs\n",
    "        return K.random_normal(tf.shape(log_var)) * K.exp(log_var/2) + mean\n",
    "\n",
    "codings_size = 50\n",
    "\n",
    "x_in = keras.layers.Input(shape=[50])\n",
    "f = keras.layers.Flatten()(x_in)\n",
    "z = keras.layers.Dense(300, activation=\"softplus\")(f)\n",
    "z = keras.layers.Dense(300, activation=\"softplus\")(z)\n",
    "\n",
    "#z = keras.layers.Dropout(0.2)(z)\n",
    "\n",
    "codings_mean = keras.layers.Dense(codings_size)(z)\n",
    "codings_log_var = keras.layers.Dense(codings_size)(z)\n",
    "codings = Sampling()([codings_mean, codings_log_var])\n",
    "\n",
    " \n",
    "\n",
    "variational_encoder = keras.models.Model(\n",
    "    inputs=[x_in], outputs=[codings_mean, codings_log_var, codings])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_classifier = keras.layers.Dense(300, activation=\"elu\")(f)\n",
    "\n",
    "#y_classifier = keras.layers.Dropout(0.2)(y_classifier)\n",
    "y_pred = keras.layers.Dense(10,activation=\"softmax\")(y_classifier) \n",
    "\n",
    "classifier = keras.models.Model(\n",
    "    inputs=[x_in], outputs=[y_pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Decoder ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_activation(value):\n",
    "    return 6*tf.keras.activations.tanh(value)\n",
    "\n",
    "latent = keras.layers.Input(shape=[codings_size])\n",
    "y = keras.layers.Input(shape=[10])\n",
    "\n",
    "l_merged = keras.layers.concatenate([latent,y])\n",
    "x = keras.layers.Dense(300, activation=\"softplus\")(l_merged)\n",
    "x = keras.layers.Dense(300, activation=\"softplus\")(x)\n",
    "\n",
    "#x = keras.layers.Dropout(0.2)(x)\n",
    "x = keras.layers.Dense(50,activation=\"sigmoid\")(x)\n",
    "x_out = keras.layers.Reshape([50])(x)\n",
    "\n",
    "\n",
    "variational_decoder = keras.models.Model(inputs=[latent,y], outputs=[x_out])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labelled vae\n",
    "_,_, codings = variational_encoder(x_in)\n",
    "y_pred = classifier(x_in)\n",
    "reconstructions = variational_decoder([codings,y])\n",
    "label_vae = keras.models.Model(inputs=(x_in,y), outputs=(reconstructions,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unlabelled vae\n",
    "_,_, codings = variational_encoder(x_in)\n",
    "y_pred = classifier(x_in)\n",
    "reconstructions_un = variational_decoder([codings,y_pred])\n",
    "unlabel_vae = keras.models.Model(inputs=x_in, outputs=reconstructions_un)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelled_loss_reconstruction(codings_log_var,codings_mean):\n",
    "    def loss_functions_labelled(x, x_decoded_mean):\n",
    "        x = K.reshape(x,[-1,50])\n",
    "        x_decoded_mean = K.reshape(x_decoded_mean,[-1,50])\n",
    "        xent_loss = 50*keras.losses.binary_crossentropy(x, x_decoded_mean)        \n",
    "        kl_loss = - 0.5 * K.sum(1 + codings_log_var - K.square(codings_mean) - K.exp(codings_log_var), axis=-1)\n",
    "        return K.mean(xent_loss + kl_loss)\n",
    "    return loss_functions_labelled \n",
    "\n",
    "def unlabelled_loss_reconstruction(codings_log_var,codings_mean,y_pred):\n",
    "    def loss_functions_unlabelled(x,x_decoded_mean):\n",
    "        x = K.reshape(x,[-1,50])\n",
    "        x_decoded_mean = K.reshape(x_decoded_mean,[-1,50])\n",
    "        kl_loss = - 0.5 * K.sum(1 + codings_log_var - K.square(codings_mean) - K.exp(codings_log_var), axis=-1)\n",
    "        xent_loss = 50*keras.losses.binary_crossentropy(x, x_decoded_mean)        \n",
    "        entropy = keras.losses.categorical_crossentropy(y_pred,y_pred)\n",
    "        loss = K.mean(kl_loss + xent_loss)\n",
    "        #need to check below. We are summing over y, but we are assuming that the loss term is independent of y\n",
    "        #which is not the case. How to do it though? https://github.com/bjlkeng/sandbox/issues/3\n",
    "        #and how to do it for regression?\n",
    "        return K.mean(K.sum(y_pred*loss,axis=-1)) + K.mean(entropy)\n",
    "    return loss_functions_unlabelled\n",
    "\n",
    "def labelled_cls_loss(y, y_pred,N=1000):\n",
    "    alpha = 0.1*N\n",
    "    cat_xent_loss = keras.losses.categorical_crossentropy(y, y_pred)\n",
    "    return alpha*K.mean(cat_xent_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_adam = keras.optimizers.Adam(learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_vae.compile(loss=[labelled_loss_reconstruction(codings_log_var,codings_mean)\n",
    "                        ,labelled_cls_loss], optimizer=optimizer_adam, experimental_run_tf_function=False)\n",
    "\n",
    "unlabel_vae.compile(loss=unlabelled_loss_reconstruction(codings_log_var,codings_mean,y_pred),\n",
    "                    optimizer=optimizer_adam, experimental_run_tf_function=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_5 (Model)                 [(None, 50), (None,  241000      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_7 (Model)                 (None, 50)           228950      model_5[1][2]                    \n",
      "                                                                 input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "model_6 (Model)                 (None, 10)           46410       input_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 516,360\n",
      "Trainable params: 516,360\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "label_vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for debugging\n",
    "\n",
    "history = label_vae.fit(\n",
    "    [X_train_la_encoded_scaled,y_train_la], [X_train_la_encoded_scaled,y_train_la], epochs=1, #batch_size=128,\n",
    "    validation_data=([X_valid_encoded_scaled,y_valid], [X_valid_encoded_scaled,y_valid])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#for debugging\n",
    "\n",
    "history = unlabel_vae.fit(\n",
    "    [X_train_un_encoded], [X_train_un_encoded], epochs=1, #batch_size=128,\n",
    "    validation_data=([X_valid_encoded], [X_valid_encoded])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create batches ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#omer method - just picks things at random so each epoch may not necessarily go through every single point.\n",
    "#but easier implementation for now\n",
    "\n",
    "def create_batch(x_label, y_label, x_unlabel, batch_s=64):\n",
    "    '''\n",
    "    Creates batches of labelled and unlabelled data. The total number of points in both batches is equal to batch_s. \n",
    "    \n",
    "    '''\n",
    "    proportion_labelled = x_label.shape[0]/(x_label.shape[0] + x_unlabel.shape[0])\n",
    "    \n",
    "    shape_label = x_label.shape[0]\n",
    "    label_per_batch = int(np.ceil(proportion_labelled*batch_s))\n",
    "    batch_idx_la = np.random.choice(list(range(shape_label)), label_per_batch)\n",
    "    batch_x_la = (x_label[batch_idx_la, :])\n",
    "    batch_y_la = (y_label[batch_idx_la])\n",
    "    \n",
    "    shape_unlabel = x_unlabel.shape[0]\n",
    "    unlabel_per_batch = batch_s - label_per_batch\n",
    "    batch_idx_un = np.random.choice(list(range(shape_unlabel)), unlabel_per_batch)\n",
    "    batch_x_un = (x_unlabel[batch_idx_un, :])\n",
    "    \n",
    "    del batch_idx_la,batch_idx_un\n",
    "            \n",
    "    return batch_x_la, batch_y_la, batch_x_un"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progress_bar(iteration, total, size=30):\n",
    "    running = iteration < total\n",
    "    c = \">\" if running else \"=\"\n",
    "    p = (size - 1) * iteration // total\n",
    "    fmt = \"{{:-{}d}}/{{}} [{{}}]\".format(len(str(total)))\n",
    "    params = [iteration, total, \"=\" * p + c + \".\" * (size - p - 1)]\n",
    "    return fmt.format(*params)\n",
    "\n",
    "def print_status_bar(iteration, total, loss, metrics=None, size=30):\n",
    "    metrics = \" - \".join([\"Loss: {:.4f}\".format(loss)])\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{} - {}\".format(progress_bar(iteration, total), metrics), end=end)\n",
    "    \n",
    "def print_status_bar_epoch(iteration, total, loss, validation_loss, metrics=None, size=30):\n",
    "    metrics = \" - \".join([\"Loss: {:.4f} Validation loss: {:.4f} \".format(loss,validation_loss)])\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{} - {}\".format(progress_bar(iteration, total), metrics), end=end)\n",
    "    \n",
    "#could make these functions into just one which works for both if I want. See the Geron textbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_average(list_of_losses):\n",
    "    return sum(list_of_losses)/len(list_of_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#for below\n",
    "def progress_bar(iteration, total, size=30):\n",
    "    running = iteration < total\n",
    "    c = \">\" if running else \"=\"\n",
    "    p = (size - 1) * iteration // total\n",
    "    fmt = \"{{:-{}d}}/{{}} [{{}}]\".format(len(str(total)))\n",
    "    params = [iteration, total, \"=\" * p + c + \".\" * (size - p - 1)]\n",
    "    return fmt.format(*params)\n",
    "\n",
    "def print_status_bar(iteration, total, loss, metrics=None, size=30):\n",
    "    metrics = \" - \".join([\"Loss for batch: {:.4f}\".format(loss)])\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{} - {}\".format(progress_bar(iteration, total), metrics), end=end)\n",
    "    \n",
    "def print_status_bar_epoch(iteration, total, training_loss_for_epoch,train_recon, train_reg,\n",
    "                                   train_unlabel,val_loss,val_recon_loss,val_reg_loss, metrics=None, size=30):\n",
    "    metrics = \" - \".join(\n",
    "        [\"trainLoss: {:.4f} tRecon: {:.4f} ty_pred: {:.4f} tUnlabel: {:.4f} Val_loss: {:.4f} Val_recon: {:.4f} Val_reg: {:.4f}\".format(\n",
    "            training_loss_for_epoch,train_recon, train_reg,\n",
    "                                   train_unlabel,val_loss,val_recon_loss,val_reg_loss)]\n",
    "    )\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{} - {}\".format(progress_bar(iteration, total), metrics), end=end)\n",
    "    \n",
    "#could make these functions into just one which works for both if I want. See the Geron textbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(X_train_la, y_train_la, X_train_un,epochs,X_valid_la, y_valid_la,patience,batch_size=64):\n",
    "\n",
    "    \"\"\"\n",
    "    Fits the model. Gets the validation loss too. And includes early stopping, given by the patience.\n",
    "    \n",
    "    \"\"\"\n",
    "    #The callback still doesn't take the model back to the model which performed best.. how to implement that?\n",
    "    #maybe save the model after each epoch in a list of max size 10.\n",
    "    #if we callback is reached, look at val loss list and see index which is smallest and choose that for model restoration\n",
    "    \n",
    "    start = time.time()\n",
    "    history = []\n",
    "    \n",
    "    validation_loss = []\n",
    "    \n",
    "    batch_loss = []\n",
    "    batch_recon = []\n",
    "    batch_reg = []\n",
    "    batch_unlabel = []\n",
    "    \n",
    "    batches_per_epoch = int(np.floor((X_train_la.shape[0] + X_train_un.shape[0])/batch_size))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "            \n",
    "            print(\"Epoch {}/{}\".format(epoch,epochs))\n",
    "            \n",
    "            for i in range(batches_per_epoch):\n",
    "\n",
    "                batch_x_la, batch_y_la, batch_x_un = create_batch(X_train_la,y_train_la,X_train_un,batch_size)\n",
    "\n",
    "                unlabel_loss = unlabel_vae.train_on_batch([batch_x_un],\n",
    "                                                 [batch_x_un])\n",
    "\n",
    "                label_loss_recon = label_vae.train_on_batch([batch_x_la,batch_y_la],\n",
    "                                                [batch_x_la,batch_y_la])[1] \n",
    "                \n",
    "                label_loss_y = label_vae.train_on_batch([batch_x_la,batch_y_la],\n",
    "                                                [batch_x_la,batch_y_la])[2] \n",
    "                \n",
    "                loss= unlabel_loss+label_loss_recon+label_loss_y\n",
    "                \n",
    "                batch_recon.append(label_loss_recon)\n",
    "                batch_reg.append(label_loss_y)\n",
    "                batch_unlabel.append(unlabel_loss)\n",
    "                \n",
    "                batch_loss.append(loss)\n",
    "                average_batch_loss = list_average(batch_loss)\n",
    "                print_status_bar(i*batch_size,X_train_la.shape[0] + X_train_un.shape[0],average_batch_loss)\n",
    "                \n",
    "            training_loss_for_epoch = list_average(batch_loss)\n",
    "            train_recon = list_average(batch_recon)\n",
    "            train_reg = list_average(batch_reg)\n",
    "            train_unlabel = list_average(batch_unlabel)\n",
    "\n",
    "            batch_loss = []\n",
    "            batch_recon = []\n",
    "            batch_reg = []\n",
    "            batch_unlabel = []\n",
    "                \n",
    "            history.append(training_loss_for_epoch)\n",
    "            \n",
    "            val_loss = label_vae.evaluate([X_valid_la,y_valid_la],[X_valid_la,y_valid_la],verbose=0)[0];\n",
    "            \n",
    "            val_recon_loss = label_vae.evaluate([X_valid_la,y_valid_la],[X_valid_la,y_valid_la],verbose=0)[1];\n",
    "\n",
    "            val_reg_loss = label_vae.evaluate([X_valid_la,y_valid_la],[X_valid_la,y_valid_la],verbose=0)[2];\n",
    "            \n",
    "            validation_loss.append(val_loss)\n",
    "                \n",
    "            print_status_bar_epoch(X_train_la.shape[0] + X_train_un.shape[0]\n",
    "                             ,(X_train_la.shape[0] + X_train_un.shape[0]),training_loss_for_epoch,train_recon, train_reg,\n",
    "                                   train_unlabel,val_loss,val_recon_loss,val_reg_loss)\n",
    "            \n",
    "            #callback for early stopping\n",
    "            if epoch > patience - 1:\n",
    "                \n",
    "                latest_val_loss = validation_loss[-patience:]\n",
    "                if all(i<=val_loss for i in latest_val_loss) is True:\n",
    "                    break\n",
    "            \n",
    "            \n",
    "                \n",
    "    done = time.time()\n",
    "    elapsed = done-start\n",
    "    print(\"Elapsed/s: \",elapsed)\n",
    "    print(\"Final training loss: \",training_loss_for_epoch)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/200\n",
      "55000/55000 [==============================] - trainLoss: 84.6722 tRecon: 34.1434 ty_pred: 16.1208 tUnlabel: 34.4080 Val_loss: 63.9988 Val_recon: 34.1533 Val_reg: 29.7790\n",
      "Epoch 1/200\n",
      "55000/55000 [==============================] - trainLoss: 83.1775 tRecon: 34.1174 ty_pred: 14.6561 tUnlabel: 34.4040 Val_loss: 64.9535 Val_recon: 34.1512 Val_reg: 30.7648\n",
      "Epoch 2/200\n",
      "55000/55000 [==============================] - trainLoss: 81.1029 tRecon: 34.1351 ty_pred: 12.5624 tUnlabel: 34.4054 Val_loss: 69.5679 Val_recon: 34.1549 Val_reg: 35.3887\n",
      "Epoch 3/200\n",
      "55000/55000 [==============================] - trainLoss: 82.4765 tRecon: 34.1218 ty_pred: 13.9584 tUnlabel: 34.3963 Val_loss: 65.5479 Val_recon: 34.1547 Val_reg: 31.2727\n",
      "Epoch 4/200\n",
      "55000/55000 [==============================] - trainLoss: 83.5292 tRecon: 34.1342 ty_pred: 15.0008 tUnlabel: 34.3943 Val_loss: 71.9349 Val_recon: 34.1515 Val_reg: 37.6621\n",
      "Epoch 5/200\n",
      "55000/55000 [==============================] - trainLoss: 83.2490 tRecon: 34.1230 ty_pred: 14.7317 tUnlabel: 34.3943 Val_loss: 64.5992 Val_recon: 34.1581 Val_reg: 30.3874\n",
      "Epoch 6/200\n",
      "55000/55000 [==============================] - trainLoss: 83.4054 tRecon: 34.1321 ty_pred: 14.8879 tUnlabel: 34.3854 Val_loss: 68.7317 Val_recon: 34.1532 Val_reg: 34.4700\n",
      "Epoch 7/200\n",
      "55000/55000 [==============================] - trainLoss: 81.5499 tRecon: 34.1247 ty_pred: 13.0419 tUnlabel: 34.3833 Val_loss: 70.1043 Val_recon: 34.1554 Val_reg: 35.8285\n",
      "Epoch 8/200\n",
      "55000/55000 [==============================] - trainLoss: 83.9251 tRecon: 34.1360 ty_pred: 15.4006 tUnlabel: 34.3885 Val_loss: 64.1956 Val_recon: 34.1569 Val_reg: 29.9463\n",
      "Epoch 9/200\n",
      "55000/55000 [==============================] - trainLoss: 82.3744 tRecon: 34.1335 ty_pred: 13.8608 tUnlabel: 34.3801 Val_loss: 71.1630 Val_recon: 34.1581 Val_reg: 36.8734\n",
      "Epoch 10/200\n",
      "55000/55000 [==============================] - trainLoss: 83.4453 tRecon: 34.1367 ty_pred: 14.9244 tUnlabel: 34.3843 Val_loss: 72.2754 Val_recon: 34.1511 Val_reg: 37.9986\n",
      "Elapsed/s:  169.7367775440216\n",
      "Final training loss:  83.4453200583408\n"
     ]
    }
   ],
   "source": [
    "history = fit_model(X_train_la_encoded_scaled, y_train_la, X_train_un_encoded_scaled,200,X_valid_encoded_scaled,y_valid,patience=10,batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 300)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.reshape(-1,1)\n",
    "y_test = encoder.transform(y_test)\n",
    "y_test = y_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction = classifier.predict(X_valid_encoded_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "xent_loss = log_loss(y_valid,y_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38124066108700844"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xent_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rounded = np.round(y_prediction,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8718"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score as accuracy\n",
    "\n",
    "accuracy_score = accuracy(y_valid,y_pred_rounded)\n",
    "\n",
    "accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rounded_train = np.round(classifier.predict(X_train_la_encoded_scaled),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "xent_loss = log_loss(y_train_la,y_pred_rounded_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2841644129753114"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xent_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.918"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y_train_la,y_pred_rounded_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not as good as M1 nor M2 yet. Maybe with regularisation + longer training we'd get there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe the models need to be trained jointly, as in having a probabilistic model overall, unlike what is done here, where M1 is used as a feature extractor? Can try this out..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save(\"m1+m2_poor_classifier.h5\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
