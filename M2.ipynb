{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "#check os.environ ld_library_path is the same here as when I do it in python via terminal, if I get issues\n",
    "\n",
    "#sometimes I can't select the GPU. In this case, try: https://forums.fast.ai/t/tip-limiting-tensorflow-to-one-gpu/1995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = keras.backend\n",
    "\n",
    "\n",
    "mnist = keras.datasets.mnist\n",
    "(X_train_full, y_train_full), (X_test,y_test) = mnist.load_data()\n",
    "\n",
    "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
    "\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "#1000 labelled points in train set for this example\n",
    "\n",
    "X_train_la = X_train[:1000]\n",
    "X_train_un = X_train[1000:]\n",
    "\n",
    "y_train_la = y_train[:1000]\n",
    "y_train_un = np.empty((X_train_la.shape[0]))\n",
    "\n",
    "\n",
    "y_train_la = y_train_la.reshape(-1,1)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "y_train_la = encoder.fit_transform(y_train_la)\n",
    "\n",
    "y_train_la=y_train_la.toarray()\n",
    "\n",
    "y_valid = y_valid.reshape(-1,1)\n",
    "y_valid = encoder.transform(y_valid)\n",
    "y_valid = y_valid.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "#reparameterization trick\n",
    "class Sampling(keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        mean, log_var = inputs\n",
    "        return K.random_normal(tf.shape(log_var)) * K.exp(log_var/2) + mean\n",
    "\n",
    "codings_size = 50\n",
    "\n",
    "x_in = keras.layers.Input(shape=[28, 28])\n",
    "f = keras.layers.Flatten()(x_in)\n",
    "z = keras.layers.Dense(600, activation=\"softplus\")(f)\n",
    "z = keras.layers.Dense(300, activation=\"softplus\")(z)\n",
    "\n",
    "codings_mean = keras.layers.Dense(codings_size)(z)\n",
    "codings_log_var = keras.layers.Dense(codings_size)(z)\n",
    "codings = Sampling()([codings_mean, codings_log_var])\n",
    "\n",
    " \n",
    "\n",
    "variational_encoder = keras.models.Model(\n",
    "    inputs=[x_in], outputs=[codings_mean, codings_log_var, codings])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_classifier = keras.layers.Dense(300, activation=\"selu\")(f)\n",
    "y_classifier = keras.layers.Dense(100, activation=\"selu\")(y_classifier)\n",
    "y_pred = keras.layers.Dense(10,activation=\"softmax\")(y_classifier) \n",
    "\n",
    "classifier = keras.models.Model(\n",
    "    inputs=[x_in], outputs=[y_pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e5557067c453>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlatent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcodings_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0ml_merged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlatent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"softplus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml_merged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "latent = keras.layers.Input(shape=[codings_size])\n",
    "y = keras.layers.Input(shape=[10])\n",
    "\n",
    "l_merged = keras.layers.concatenate([latent,y])\n",
    "x = keras.layers.Dense(300, activation=\"softplus\")(l_merged)\n",
    "x = keras.layers.Dense(600, activation=\"softplus\")(x)\n",
    "x = keras.layers.Dense(28 * 28, activation=\"sigmoid\")(x)\n",
    "x_out = keras.layers.Reshape([28,28])(x)\n",
    "\n",
    "\n",
    "variational_decoder = keras.models.Model(inputs=[latent,y], outputs=[x_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 60)           0           input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 300)          18300       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 600)          180600      dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 784)          471184      dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 28, 28)       0           dense_9[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 670,084\n",
      "Trainable params: 670,084\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "variational_decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile model# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labelled vae\n",
    "_,_, codings = variational_encoder(x_in)\n",
    "y_pred = classifier(x_in)\n",
    "reconstructions = variational_decoder([codings,y])\n",
    "label_vae = keras.models.Model(inputs=(x_in,y), outputs=(reconstructions,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unlabelled vae\n",
    "_,_, codings = variational_encoder(x_in)\n",
    "y_pred = classifier(x_in)\n",
    "reconstructions_un = variational_decoder([codings,y_pred])\n",
    "unlabel_vae = keras.models.Model(inputs=x_in, outputs=reconstructions_un)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelled_loss_reconstruction(codings_log_var,codings_mean):\n",
    "    def loss_functions_labelled(x, x_decoded_mean):\n",
    "        x = K.reshape(x,[-1,28*28])\n",
    "        x_decoded_mean = K.reshape(x_decoded_mean,[-1,28*28])\n",
    "        xent_loss = 28*28*keras.losses.binary_crossentropy(x, x_decoded_mean)        \n",
    "        kl_loss = - 0.5 * K.sum(1 + codings_log_var - K.square(codings_mean) - K.exp(codings_log_var), axis=-1)\n",
    "        return K.mean(xent_loss + kl_loss)\n",
    "    return loss_functions_labelled \n",
    "\n",
    "def unlabelled_loss_reconstruction(codings_log_var,codings_mean,y_pred):\n",
    "    def loss_functions_unlabelled(x,x_decoded_mean):\n",
    "        x = K.reshape(x,[-1,28*28])\n",
    "        x_decoded_mean = K.reshape(x_decoded_mean,[-1,28*28])\n",
    "        kl_loss = - 0.5 * K.sum(1 + codings_log_var - K.square(codings_mean) - K.exp(codings_log_var), axis=-1)\n",
    "        xent_loss = 28*28*keras.losses.binary_crossentropy(x, x_decoded_mean)        \n",
    "        entropy = keras.losses.categorical_crossentropy(y_pred,y_pred)\n",
    "        loss = K.mean(kl_loss + xent_loss)\n",
    "        #need to check below. We are summing over y, but we are assuming that the loss term is independent of y\n",
    "        #which is not the case. How to do it though? https://github.com/bjlkeng/sandbox/issues/3\n",
    "        #and how to do it for regression?\n",
    "        return K.mean(K.sum(y_pred*loss,axis=-1)) + K.mean(entropy)\n",
    "    return loss_functions_unlabelled\n",
    "\n",
    "def labelled_cls_loss(y, y_pred,N=1000):\n",
    "    alpha = 0.1*N\n",
    "    cat_xent_loss = keras.losses.categorical_crossentropy(y, y_pred)\n",
    "    return alpha*K.mean(cat_xent_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_vae.compile(loss=[labelled_loss_reconstruction(codings_log_var,codings_mean)\n",
    "                        ,labelled_cls_loss], optimizer=\"rmsprop\", experimental_run_tf_function=False)\n",
    "\n",
    "unlabel_vae.compile(loss=unlabelled_loss_reconstruction(codings_log_var,codings_mean,y_pred),\n",
    "                    optimizer=\"rmsprop\", experimental_run_tf_function=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "1000/1000 [==============================] - 1s 850us/sample - loss: 239.7813 - model_2_loss: 207.9339 - model_1_loss: 32.3638 - val_loss: 294.5925 - val_model_2_loss: 201.5856 - val_model_1_loss: 93.1992\n",
      "Epoch 2/2\n",
      "1000/1000 [==============================] - 0s 403us/sample - loss: 217.6201 - model_2_loss: 198.5067 - model_1_loss: 19.4587 - val_loss: 261.4440 - val_model_2_loss: 198.8071 - val_model_1_loss: 62.4398\n"
     ]
    }
   ],
   "source": [
    "#for debugging\n",
    "\n",
    "history = label_vae.fit(\n",
    "    [X_train_la,y_train_la], [X_train_la,y_train_la], epochs=2, #batch_size=128,\n",
    "    validation_data=([X_valid,y_valid], [X_valid,y_valid])\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for debugging\n",
    "\n",
    "history = unlabel_vae.fit(\n",
    "    [X_train_un], [X_train_un], epochs=2, #batch_size=128,\n",
    "    validation_data=([X_valid], [X_valid])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create mini-batches # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not the official implementation, as it performs the gradient update after each batch of labelled and then after each batch of unlabelled (as opposed to calculating the loss for everything in the batch and then performing the update on it).\n",
    "\n",
    "Omer's implementation may help with doing the gradient updates properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53991.0\n"
     ]
    }
   ],
   "source": [
    "#This method hasn't been finished by me. See below.\n",
    "batch_size = 64\n",
    "proportion_unlabelled = X_train_un.shape[0]/(X_train_la.shape[0] + X_train_un.shape[0])\n",
    "proportion_unlabelled\n",
    "\n",
    "unlabelled_points_per_batch = np.ceil((proportion_unlabelled*batch_size))\n",
    "\n",
    "labelled_points_per_batch = batch_size - unlabelled_points_per_batch\n",
    "\n",
    "max_batches_with_labelled_points_per_batch = np.floor(X_train_la.shape[0]/labelled_points_per_batch)\n",
    "print(max_batches_with_labelled_points_per_batch)\n",
    "\n",
    "max_batches_with_unlabelled_points_per_batch = np.floor(X_train_un.shape[0]/unlabelled_points_per_batch)\n",
    "print(max_batches_with_unlabelled_points_per_batch)\n",
    "\n",
    "num_complete_batches = int(np.floor((X_train_la.shape[0] + X_train_un.shape[0])/batch_size)) #regardless of composition\n",
    "print(num_complete_batches)\n",
    "\n",
    "#unlabelled points\n",
    "number_of_points_in_complete_batches = max_batches_with_unlabelled_points_per_batch*unlabelled_points_per_batch\n",
    "print(number_of_points_in_complete_batches)\n",
    "\n",
    "max_full_batch = np.max(max_batches_with_unlabelled_points_per_batch,max_batches_with_labelled_points_per_batch)\n",
    "\n",
    "for i in range(max_full_batch):\n",
    "    index_start = i*unlabelled_points_per_batch\n",
    "    index_end = (i+1)*unlabelled_points_per_batch\n",
    "    batch_unlabelled_X = X_train_un[index_start:index_end]\n",
    "    loss = label_vae.train_on_batch(batch....)\n",
    "    last_index_unlabelled = index_end\n",
    "    \n",
    "    index_start2 = i*labelled_points_per_batch\n",
    "    index_end2 = (i+1)*labelled_points_per_batch\n",
    "    batch_labelled_X = X_train_la[index_start2:index_end2]\n",
    "    batch_labelled_y = y_train_la[index_start2:index_end2]\n",
    "    loss += unlabelled_vae.train_on_batch(....)\n",
    "    last_index_labelled = index_end2\n",
    "    \n",
    "#this will train it on batches that are 'full'\n",
    "\n",
    "#then need to deal with the remaining \n",
    "\n",
    "if max_full_batch*unlabelled_points_per_batch < X_train_un.shape[0]:\n",
    "    if X_train_un.shape[0] - max_full_batch*unlabelled_points_per_batch <= batch_size: #then just one batch remaining\n",
    "        batch = X_train_un[last_index:]\n",
    "        loss += label_vae.train_on_batch(batch....)\n",
    "    else:\n",
    "        #make batches and then train on the remaining ones\n",
    "        \n",
    "#repeat for unlabelled\n",
    "        \n",
    "\n",
    "#does train on batch shuffle the batches? if not then I need to shuffle after each epoch. can just shuffle the indices\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#omer method - just picks things at random so each epoch may not necessarily go through every single point.\n",
    "#but easier implementation for now\n",
    "\n",
    "def create_batch(x_label, y_label, x_unlabel, batch_s=64):\n",
    "    '''\n",
    "    Creates batches of labelled and unlabelled data. The total number of points in both batches is equal to batch_s. \n",
    "    \n",
    "    '''\n",
    "    proportion_labelled = x_label.shape[0]/(x_label.shape[0] + x_unlabel.shape[0])\n",
    "    \n",
    "    shape_label = x_label.shape[0]\n",
    "    label_per_batch = int(np.ceil(proportion_labelled*batch_s))\n",
    "    batch_idx_la = np.random.choice(list(range(shape_label)), label_per_batch)\n",
    "    batch_x_la = (x_label[batch_idx_la, :])\n",
    "    batch_y_la = (y_label[batch_idx_la])\n",
    "    \n",
    "    shape_unlabel = x_unlabel.shape[0]\n",
    "    unlabel_per_batch = batch_s - label_per_batch\n",
    "    batch_idx_un = np.random.choice(list(range(shape_unlabel)), unlabel_per_batch)\n",
    "    batch_x_un = (x_unlabel[batch_idx_un, :])\n",
    "    \n",
    "    del batch_idx_la,batch_idx_un\n",
    "            \n",
    "    return batch_x_la, batch_y_la, batch_x_un"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progress_bar(iteration, total, size=30):\n",
    "    running = iteration < total\n",
    "    c = \">\" if running else \"=\"\n",
    "    p = (size - 1) * iteration // total\n",
    "    fmt = \"{{:-{}d}}/{{}} [{{}}]\".format(len(str(total)))\n",
    "    params = [iteration, total, \"=\" * p + c + \".\" * (size - p - 1)]\n",
    "    return fmt.format(*params)\n",
    "\n",
    "def print_status_bar(iteration, total, loss, metrics=None, size=30):\n",
    "    metrics = \" - \".join([\"Loss for batch: {:.4f}\".format(loss)])\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{} - {}\".format(progress_bar(iteration, total), metrics), end=end)\n",
    "    \n",
    "def print_status_bar_epoch(iteration, total, loss, validation_loss, metrics=None, size=30):\n",
    "    metrics = \" - \".join([\"Loss: {:.4f} Validation loss: {:.4f} \".format(loss,validation_loss)])\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{} - {}\".format(progress_bar(iteration, total), metrics), end=end)\n",
    "    \n",
    "#could make these functions into just one which works for both if I want. See the Geron textbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(X_train_la, y_train_la, X_train_un,epochs,X_valid_la, y_valid_la,patience,batch_size=64):\n",
    "\n",
    "    \"\"\"\n",
    "    Fits the model. Gets the validation loss too. And includes early stopping, given by the patience.\n",
    "    \n",
    "    \"\"\"\n",
    "    #The callback still doesn't take the model back to the model which performed best.. how to implement that?\n",
    "    #maybe save the model after each epoch in a list of max size 10.\n",
    "    #if we callback is reached, look at val loss list and see index which is smallest and choose that for model restoration\n",
    "    \n",
    "    start = time.time()\n",
    "    history = []\n",
    "    \n",
    "    validation_loss = []\n",
    "    \n",
    "    batches_per_epoch = int(np.floor((X_train_la.shape[0] + X_train_un.shape[0])/batch_size))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "            \n",
    "            print(\"Epoch {}/{}\".format(epoch,epochs))\n",
    "            \n",
    "            for i in range(batches_per_epoch):\n",
    "\n",
    "                batch_x_la, batch_y_la, batch_x_un = create_batch(X_train_la,y_train_la,X_train_un,batch_size)\n",
    "\n",
    "                loss = unlabel_vae.train_on_batch(batch_x_un,batch_x_un)\n",
    "\n",
    "                loss += label_vae.train_on_batch([batch_x_la,batch_y_la],\n",
    "                                                [batch_x_la,batch_y_la])[0] #selecting the overall loss term\n",
    "                \n",
    "\n",
    "                history.append(loss)\n",
    "                print_status_bar(i*batch_size,X_train_la.shape[0] + X_train_un.shape[0],loss)\n",
    "                \n",
    "            #val_loss = unlabel_vae.evaluate(X_train_un,X_train_un,verbose=0);\n",
    "            \n",
    "            val_loss = label_vae.evaluate([X_valid_la,y_valid_la],[X_valid_la,y_valid_la],verbose=0)[0];\n",
    "            \n",
    "            validation_loss.append(val_loss)\n",
    "                \n",
    "            print_status_bar_epoch(X_train_la.shape[0] + X_train_un.shape[0]\n",
    "                             ,(X_train_la.shape[0] + X_train_un.shape[0]),loss,val_loss)\n",
    "            \n",
    "            #callback for early stopping\n",
    "            if epoch > patience - 1:\n",
    "                \n",
    "                latest_val_loss = validation_loss[-patience:]\n",
    "                if all(i<=val_loss for i in latest_val_loss) is True:\n",
    "                    break\n",
    "            \n",
    "            \n",
    "                \n",
    "    done = time.time()\n",
    "    elapsed = done-start\n",
    "    print(\"Elapsed: \",elapsed)\n",
    "    print(\"Final training loss: \",loss)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/200\n",
      "55000/55000 [==============================] - Loss: 306.6883 Validation loss: 283.5929 \n",
      "Epoch 1/200\n",
      "55000/55000 [==============================] - Loss: 236.4263 Validation loss: 262.6542 \n",
      "Epoch 2/200\n",
      "55000/55000 [==============================] - Loss: 249.4685 Validation loss: 252.5325 \n",
      "Epoch 3/200\n",
      "55000/55000 [==============================] - Loss: 249.6751 Validation loss: 247.5166 \n",
      "Epoch 4/200\n",
      "55000/55000 [==============================] - Loss: 247.4236 Validation loss: 240.9110 \n",
      "Epoch 5/200\n",
      "55000/55000 [==============================] - Loss: 250.3364 Validation loss: 234.6063 \n",
      "Epoch 6/200\n",
      "55000/55000 [==============================] - Loss: 239.2738 Validation loss: 231.1337 \n",
      "Epoch 7/200\n",
      "55000/55000 [==============================] - Loss: 250.0407 Validation loss: 231.0512 \n",
      "Epoch 8/200\n",
      "55000/55000 [==============================] - Loss: 264.0227 Validation loss: 227.6274 \n",
      "Epoch 9/200\n",
      "55000/55000 [==============================] - Loss: 215.9293 Validation loss: 224.3265 \n",
      "Epoch 10/200\n",
      "55000/55000 [==============================] - Loss: 204.4451 Validation loss: 223.6103 \n",
      "Epoch 11/200\n",
      "55000/55000 [==============================] - Loss: 237.8082 Validation loss: 221.4716 \n",
      "Epoch 12/200\n",
      "55000/55000 [==============================] - Loss: 206.4460 Validation loss: 224.2518 \n",
      "Epoch 13/200\n",
      "55000/55000 [==============================] - Loss: 206.7593 Validation loss: 222.9356 \n",
      "Epoch 14/200\n",
      "55000/55000 [==============================] - Loss: 219.5237 Validation loss: 224.0241 \n",
      "Epoch 15/200\n",
      "55000/55000 [==============================] - Loss: 204.1999 Validation loss: 220.7480 \n",
      "Epoch 16/200\n",
      "55000/55000 [==============================] - Loss: 198.7756 Validation loss: 221.4649 \n",
      "Epoch 17/200\n",
      "55000/55000 [==============================] - Loss: 215.3410 Validation loss: 222.9129 \n",
      "Epoch 18/200\n",
      "55000/55000 [==============================] - Loss: 253.1352 Validation loss: 224.2221 \n",
      "Epoch 19/200\n",
      "55000/55000 [==============================] - Loss: 227.8640 Validation loss: 225.2689 \n",
      "Elapsed:  307.0948393344879\n",
      "Final training loss:  227.86403\n"
     ]
    }
   ],
   "source": [
    "history = fit_model(X_train_la, y_train_la, X_train_un,200,X_valid,y_valid,patience=10,batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.reshape(-1,1)\n",
    "y_test = encoder.transform(y_test)\n",
    "y_test = y_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "xent_loss = log_loss(y_test,y_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29353658141897493"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xent_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rounded = np.round(y_prediction,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score as accuracy\n",
    "\n",
    "accuracy_score = accuracy(y_test,y_pred_rounded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9355"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performs better than PCA + SVM, YES! ##"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
