{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "assert sys.version_info >= (3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "#check os.environ ld_library_path is the same here as when I do it in python via terminal, if I get issues\n",
    "\n",
    "#sometimes I can't select the GPU. In this case, try: https://forums.fast.ai/t/tip-limiting-tensorflow-to-one-gpu/1995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = keras.backend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = keras.datasets.mnist\n",
    "(X_train_full, y_train_full), (X_test,y_test) = mnist.load_data()\n",
    "\n",
    "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
    "\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "#1000 labelled points in train set for this example\n",
    "\n",
    "X_train_la = X_train[:1000]\n",
    "X_train_un = X_train[1000:]\n",
    "\n",
    "y_train_la = y_train[:1000]\n",
    "y_train_un = np.empty((X_train_la.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_la = y_train_la.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "y_train_la = encoder.fit_transform(y_train_la)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_la=y_train_la.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid = y_valid.reshape(-1,1)\n",
    "y_valid = encoder.transform(y_valid)\n",
    "y_valid = y_valid.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "#reparameterization trick\n",
    "class Sampling(keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        mean, log_var = inputs\n",
    "        return K.random_normal(tf.shape(log_var)) * K.exp(log_var/2) + mean\n",
    "\n",
    "codings_size = 50\n",
    "\n",
    "x_in = keras.layers.Input(shape=[28, 28])\n",
    "f = keras.layers.Flatten()(x_in)\n",
    "z = keras.layers.Dense(600, activation=\"softplus\")(f)\n",
    "z = keras.layers.Dense(300, activation=\"softplus\")(z)\n",
    "\n",
    "codings_mean = keras.layers.Dense(codings_size)(z)\n",
    "codings_log_var = keras.layers.Dense(codings_size)(z)\n",
    "codings = Sampling()([codings_mean, codings_log_var])\n",
    "\n",
    " \n",
    "\n",
    "variational_encoder = keras.models.Model(\n",
    "    inputs=[x_in], outputs=[codings_mean, codings_log_var, codings])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_classifier = keras.layers.Dense(300, activation=\"selu\")(f)\n",
    "y_classifier = keras.layers.Dense(100, activation=\"selu\")(y_classifier)\n",
    "y_pred = keras.layers.Dense(10,activation=\"softmax\")(y_classifier) \n",
    "\n",
    "classifier = keras.models.Model(\n",
    "    inputs=[x_in], outputs=[y_pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "latent = keras.layers.Input(shape=[codings_size])\n",
    "y = keras.layers.Input(shape=[10])\n",
    "\n",
    "l_merged = keras.layers.concatenate([latent,y])\n",
    "x = keras.layers.Dense(300, activation=\"softplus\")(l_merged)\n",
    "x = keras.layers.Dense(600, activation=\"softplus\")(x)\n",
    "x = keras.layers.Dense(28 * 28, activation=\"sigmoid\")(x)\n",
    "x_out = keras.layers.Reshape([28,28])(x)\n",
    "\n",
    "\n",
    "variational_decoder = keras.models.Model(inputs=[latent,y], outputs=[x_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 60)           0           input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 300)          18300       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 600)          180600      dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 784)          471184      dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 28, 28)       0           dense_9[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 670,084\n",
      "Trainable params: 670,084\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "variational_decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile model# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labelled vae\n",
    "_,_, codings = variational_encoder(x_in)\n",
    "y_pred = classifier(x_in)\n",
    "reconstructions = variational_decoder([codings,y])\n",
    "label_vae = keras.models.Model(inputs=(x_in,y), outputs=(reconstructions,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unlabelled vae\n",
    "_,_, codings = variational_encoder(x_in)\n",
    "y_pred = classifier(x_in)\n",
    "reconstructions_un = variational_decoder([codings,y_pred])\n",
    "unlabel_vae = keras.models.Model(inputs=x_in, outputs=reconstructions_un)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelled_loss_reconstruction(codings_log_var,codings_mean):\n",
    "    def loss_functions_labelled(x, x_decoded_mean):\n",
    "        x = K.reshape(x,[-1,28*28])\n",
    "        x_decoded_mean = K.reshape(x_decoded_mean,[-1,28*28])\n",
    "        xent_loss = 28*28*keras.losses.binary_crossentropy(x, x_decoded_mean)        \n",
    "        kl_loss = - 0.5 * K.sum(1 + codings_log_var - K.square(codings_mean) - K.exp(codings_log_var), axis=-1)\n",
    "        return K.mean(xent_loss + kl_loss)\n",
    "    return loss_functions_labelled \n",
    "\n",
    "def unlabelled_loss_reconstruction(codings_log_var,codings_mean,y_pred):\n",
    "    def loss_functions_unlabelled(x,x_decoded_mean):\n",
    "        x = K.reshape(x,[-1,28*28])\n",
    "        x_decoded_mean = K.reshape(x_decoded_mean,[-1,28*28])\n",
    "        kl_loss = - 0.5 * K.sum(1 + codings_log_var - K.square(codings_mean) - K.exp(codings_log_var), axis=-1)\n",
    "        xent_loss = 28*28*keras.losses.binary_crossentropy(x, x_decoded_mean)        \n",
    "        entropy = keras.losses.categorical_crossentropy(y_pred,y_pred)\n",
    "        loss = K.mean(kl_loss + xent_loss)\n",
    "        #need to check below. We are summing over y, but we are assuming that the loss term is independent of y\n",
    "        #which is not the case. How to do it though? https://github.com/bjlkeng/sandbox/issues/3\n",
    "        #and how to do it for regression?\n",
    "        return K.mean(K.sum(y_pred*loss,axis=-1)) + K.mean(entropy)\n",
    "    return loss_functions_unlabelled\n",
    "\n",
    "def labelled_cls_loss(y, y_pred,N=1000):\n",
    "    alpha = 0.1*N\n",
    "    cat_xent_loss = keras.losses.categorical_crossentropy(y, y_pred)\n",
    "    return alpha*K.mean(cat_xent_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_vae.compile(loss=[labelled_loss_reconstruction(codings_log_var,codings_mean)\n",
    "                        ,labelled_cls_loss], optimizer=\"rmsprop\", experimental_run_tf_function=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabel_vae.compile(loss=unlabelled_loss_reconstruction(codings_log_var,codings_mean,y_pred),\n",
    "                    optimizer=\"rmsprop\", experimental_run_tf_function=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "1000/1000 [==============================] - 1s 850us/sample - loss: 239.7813 - model_2_loss: 207.9339 - model_1_loss: 32.3638 - val_loss: 294.5925 - val_model_2_loss: 201.5856 - val_model_1_loss: 93.1992\n",
      "Epoch 2/2\n",
      "1000/1000 [==============================] - 0s 403us/sample - loss: 217.6201 - model_2_loss: 198.5067 - model_1_loss: 19.4587 - val_loss: 261.4440 - val_model_2_loss: 198.8071 - val_model_1_loss: 62.4398\n"
     ]
    }
   ],
   "source": [
    "#for debugging\n",
    "\n",
    "history = label_vae.fit(\n",
    "    [X_train_la,y_train_la], [X_train_la,y_train_la], epochs=2, #batch_size=128,\n",
    "    validation_data=([X_valid,y_valid], [X_valid,y_valid])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "54000/54000 [==============================] - 8s 156us/sample - loss: 149.3410 - val_loss: 130.7384\n",
      "Epoch 2/2\n",
      "54000/54000 [==============================] - 8s 147us/sample - loss: 123.6484 - val_loss: 118.3713\n"
     ]
    }
   ],
   "source": [
    "#for debugging\n",
    "\n",
    "history = unlabel_vae.fit(\n",
    "    [X_train_un], [X_train_un], epochs=2, #batch_size=128,\n",
    "    validation_data=([X_valid], [X_valid])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create mini-batches # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not the official implementation, as it performs the gradient update after each batch of labelled and then after each batch of unlabelled (as opposed to calculating the loss for everything in the batch and then performing the update on it).\n",
    "\n",
    "Omer's implementation may help with doing the gradient updates properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53991.0\n"
     ]
    }
   ],
   "source": [
    "#This method hasn't been finished by me. See below.\n",
    "batch_size = 64\n",
    "proportion_unlabelled = X_train_un.shape[0]/(X_train_la.shape[0] + X_train_un.shape[0])\n",
    "proportion_unlabelled\n",
    "\n",
    "unlabelled_points_per_batch = np.ceil((proportion_unlabelled*batch_size))\n",
    "\n",
    "labelled_points_per_batch = batch_size - unlabelled_points_per_batch\n",
    "\n",
    "max_batches_with_labelled_points_per_batch = np.floor(X_train_la.shape[0]/labelled_points_per_batch)\n",
    "print(max_batches_with_labelled_points_per_batch)\n",
    "\n",
    "max_batches_with_unlabelled_points_per_batch = np.floor(X_train_un.shape[0]/unlabelled_points_per_batch)\n",
    "print(max_batches_with_unlabelled_points_per_batch)\n",
    "\n",
    "num_complete_batches = int(np.floor((X_train_la.shape[0] + X_train_un.shape[0])/batch_size)) #regardless of composition\n",
    "print(num_complete_batches)\n",
    "\n",
    "#unlabelled points\n",
    "number_of_points_in_complete_batches = max_batches_with_unlabelled_points_per_batch*unlabelled_points_per_batch\n",
    "print(number_of_points_in_complete_batches)\n",
    "\n",
    "max_full_batch = np.max(max_batches_with_unlabelled_points_per_batch,max_batches_with_labelled_points_per_batch)\n",
    "\n",
    "for i in range(max_full_batch):\n",
    "    index_start = i*unlabelled_points_per_batch\n",
    "    index_end = (i+1)*unlabelled_points_per_batch\n",
    "    batch_unlabelled_X = X_train_un[index_start:index_end]\n",
    "    loss = label_vae.train_on_batch(batch....)\n",
    "    last_index_unlabelled = index_end\n",
    "    \n",
    "    index_start2 = i*labelled_points_per_batch\n",
    "    index_end2 = (i+1)*labelled_points_per_batch\n",
    "    batch_labelled_X = X_train_la[index_start2:index_end2]\n",
    "    batch_labelled_y = y_train_la[index_start2:index_end2]\n",
    "    loss += unlabelled_vae.train_on_batch(....)\n",
    "    last_index_labelled = index_end2\n",
    "    \n",
    "#this will train it on batches that are 'full'\n",
    "\n",
    "#then need to deal with the remaining \n",
    "\n",
    "if max_full_batch*unlabelled_points_per_batch < X_train_un.shape[0]:\n",
    "    if X_train_un.shape[0] - max_full_batch*unlabelled_points_per_batch <= batch_size: #then just one batch remaining\n",
    "        batch = X_train_un[last_index:]\n",
    "        loss += label_vae.train_on_batch(batch....)\n",
    "    else:\n",
    "        #make batches and then train on the remaining ones\n",
    "        \n",
    "#repeat for unlabelled\n",
    "        \n",
    "\n",
    "#does train on batch shuffle the batches? if not then I need to shuffle after each epoch. can just shuffle the indices\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#omer method - just picks things at random so each epoch may not necessarily go through every single point.\n",
    "#but easier implementation for now\n",
    "\n",
    "def create_batch(x_label, y_label, x_unlabel, batch_s=64):\n",
    "    '''\n",
    "    Creates batches of labelled and unlabelled data. The total number of points in both batches is equal to batch_s. \n",
    "    \n",
    "    '''\n",
    "    proportion_labelled = x_label.shape[0]/(x_label.shape[0] + x_unlabel.shape[0])\n",
    "    \n",
    "    shape_label = x_label.shape[0]\n",
    "    label_per_batch = int(np.ceil(proportion_labelled*batch_s))\n",
    "    batch_idx_la = np.random.choice(list(range(shape_label)), label_per_batch)\n",
    "    batch_x_la = (x_label[batch_idx_la, :])\n",
    "    batch_y_la = (y_label[batch_idx_la])\n",
    "    \n",
    "    shape_unlabel = x_unlabel.shape[0]\n",
    "    unlabel_per_batch = batch_s - label_per_batch\n",
    "    batch_idx_un = np.random.choice(list(range(shape_unlabel)), unlabel_per_batch)\n",
    "    batch_x_un = (x_unlabel[batch_idx_un, :])\n",
    "                          \n",
    "            \n",
    "    return batch_x_la, batch_y_la, batch_x_un"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progress_bar(iteration, total, size=30):\n",
    "    running = iteration < total\n",
    "    c = \">\" if running else \"=\"\n",
    "    p = (size - 1) * iteration // total\n",
    "    fmt = \"{{:-{}d}}/{{}} [{{}}]\".format(len(str(total)))\n",
    "    params = [iteration, total, \"=\" * p + c + \".\" * (size - p - 1)]\n",
    "    return fmt.format(*params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_status_bar(iteration, total, loss, metrics=None, size=30):\n",
    "    metrics = \" - \".join([\"Loss for batch: {:.4f}\".format(loss)])\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{} - {}\".format(progress_bar(iteration, total), metrics), end=end)\n",
    "    \n",
    "def print_status_bar_epoch(iteration, total, loss, validation_loss, metrics=None, size=30):\n",
    "    metrics = \" - \".join([\"Loss: {:.4f} Validation loss: {:.4f} \".format(loss,validation_loss)])\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{} - {}\".format(progress_bar(iteration, total), metrics), end=end)\n",
    "    \n",
    "#could make these functions into just one which works for both if I want. See the Geron textbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(X_train_la, y_train_la, X_train_un,epochs,X_valid_la, y_valid_la,batch_size=64):\n",
    "\n",
    "    \n",
    "    start = time.time()\n",
    "    history = []\n",
    "    \n",
    "    validation_loss = []\n",
    "    \n",
    "    batches_per_epoch = int(np.floor((X_train_la.shape[0] + X_train_un.shape[0])/batch_size))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "            \n",
    "            print(\"Epoch {}/{}\".format(epoch,epochs))\n",
    "            \n",
    "            for i in range(batches_per_epoch):\n",
    "\n",
    "                batch_x_la, batch_y_la, batch_x_un = create_batch(X_train_la,y_train_la,X_train_un,batch_size)\n",
    "\n",
    "                loss = unlabel_vae.train_on_batch(batch_x_un,batch_x_un)\n",
    "\n",
    "                loss += label_vae.train_on_batch([batch_x_la,batch_y_la],\n",
    "                                                [batch_x_la,batch_y_la])[0] #selecting the overall loss term\n",
    "                \n",
    "\n",
    "                history.append(loss)\n",
    "                print_status_bar(i*batch_size,X_train_la.shape[0] + X_train_un.shape[0],loss)\n",
    "                \n",
    "            val_loss = unlabel_vae.evaluate(X_train_un,X_train_un,verbose=0);\n",
    "            \n",
    "            val_loss += label_vae.evaluate([X_train_la,y_train_la],[X_train_la,y_train_la],verbose=0)[0];\n",
    "            \n",
    "            validation_loss.append(val_loss)\n",
    "                \n",
    "            print_status_bar_epoch(X_train_la.shape[0] + X_train_un.shape[0]\n",
    "                             ,(X_train_la.shape[0] + X_train_un.shape[0]),loss,val_loss)\n",
    "            \n",
    "            #need to check this callback\n",
    "            patience = 10\n",
    "            latest_val_loss = validation_loss[:-patience]\n",
    "            if all(i<=val_loss for i in latest_val_loss) is True:\n",
    "                break\n",
    "            \n",
    "            \n",
    "                \n",
    "    done = time.time()\n",
    "    elapsed = done-start\n",
    "    print(\"Elapsed: \",elapsed)\n",
    "    print(\"Final training loss: \",loss)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1\n",
      "55000/55000 [==============================] - Loss: 194.0622 Validation loss: 237.6626 \n",
      "Elapsed:  15.52076244354248\n",
      "Final training loss:  194.06216\n"
     ]
    }
   ],
   "source": [
    "history = fit_model(X_train_la, y_train_la, X_train_un,1,X_valid,y_valid,batch_size=64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
